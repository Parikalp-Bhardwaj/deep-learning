{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_With_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGIqwiR5pmyX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from torch.utils.data import TensorDataset,DataLoader,Dataset\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ab4wi278uN6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_Cafo6FzqBNP",
        "outputId": "ef4a07db-f378-417e-f83a-005e0e5f1f8d"
      },
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/dataset/Celsius to Fahrenheit/training.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Celsius</th>\n",
              "      <th>Fahrenheit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1169</td>\n",
              "      <td>2136.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1280</td>\n",
              "      <td>2336.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2380</td>\n",
              "      <td>4316.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1508</td>\n",
              "      <td>2746.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>3659.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Celsius  Fahrenheit\n",
              "0     1169      2136.2\n",
              "1     1280      2336.0\n",
              "2     2380      4316.0\n",
              "3     1508      2746.4\n",
              "4     2015      3659.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fp4NsYB6dx1"
      },
      "source": [
        "data[\"Fahrenheit\"] = data[\"Fahrenheit\"].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyBvll6UqmdL",
        "outputId": "fd23b80c-41eb-49cd-e881-7a2bd04e45df"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egW6B5Aoqo59",
        "outputId": "08805e1a-3f2e-4b62-fab9-90d5913bd130"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Celsius       0\n",
              "Fahrenheit    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g35wM5EXrTEz"
      },
      "source": [
        "x=data.iloc[:,0].values\n",
        "y=data.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cafZHxcS3JnH",
        "outputId": "89259b55-de36-47a2-85ab-3e221be06dc4"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1169, 1280, 2380, ..., 1584, 2396,    0])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpjL__33Zk4T",
        "outputId": "45c22c38-d586-45ea-c519-a9276ca06a6a"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2136, 2336, 4316, ..., 2883, 4344,   32])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv-7cxxnrTS0"
      },
      "source": [
        "x_torch=torch.from_numpy(x.astype(np.float32)).view(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHtWCqCBBJi-"
      },
      "source": [
        "y_torch=torch.from_numpy(y.astype(np.float32)).view(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thl43aPJBZ3h",
        "outputId": "843efd64-de46-4ed8-b625-c8bb87b56fb1"
      },
      "source": [
        "x_torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1169.],\n",
              "        [1280.],\n",
              "        [2380.],\n",
              "        ...,\n",
              "        [1584.],\n",
              "        [2396.],\n",
              "        [   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMvTvT1TBrL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puQxITQ0BrOq",
        "outputId": "3f5cfa62-3cfc-4563-af1b-8ccd17a12380"
      },
      "source": [
        "y_torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2136.],\n",
              "        [2336.],\n",
              "        [4316.],\n",
              "        ...,\n",
              "        [2883.],\n",
              "        [4344.],\n",
              "        [  32.]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8MjjYukqqzD"
      },
      "source": [
        "min_max=StandardScaler()\n",
        "min_max2=StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmEqIuHorSc6"
      },
      "source": [
        "X=min_max.fit_transform(x.reshape(-1,1))\n",
        "Y=min_max2.fit_transform(y.reshape(-1,1))\n",
        "# Y=min_max2.fit_transform(y.reshape(len(y),1))[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drfuQktVrwTt",
        "outputId": "30df62d4-82b1-4ed4-dcb5-858609b09769"
      },
      "source": [
        "X[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.11804244],\n",
              "       [ 0.03478369],\n",
              "       [ 1.54927691],\n",
              "       ...,\n",
              "       [-0.68528899],\n",
              "       [ 0.45333454],\n",
              "       [ 1.5713059 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm4dsgpG25A-",
        "outputId": "89d3c3a6-cbfd-44ff-859f-3012985b20e8"
      },
      "source": [
        "Y[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.1178899 ],\n",
              "       [ 0.03508926],\n",
              "       [ 1.54958296],\n",
              "       ...,\n",
              "       [-0.68544259],\n",
              "       [ 0.45348727],\n",
              "       [ 1.57100004]])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXL2Yw9qr6N6"
      },
      "source": [
        "X_train=torch.from_numpy(X.astype(np.float32)).view(-1,1)\n",
        "Y_train=torch.from_numpy(Y.astype(np.float32)).view(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAhnF9QYKGHm"
      },
      "source": [
        "tensor_data=TensorDataset(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuzaGn1WKC8e"
      },
      "source": [
        "trainloader=DataLoader(tensor_data,batch_size=3,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV2uMAIELkhV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSGxMpH3sONr",
        "outputId": "ea8b0e5c-02ed-41f0-f628-d2fa0acdc8c7"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1180],\n",
              "        [ 0.0348],\n",
              "        [ 1.5493],\n",
              "        ...,\n",
              "        [ 0.4533],\n",
              "        [ 1.5713],\n",
              "        [-1.7275]])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5S876mmJJUH",
        "outputId": "77be72e8-95ef-4a87-9b9e-62d582b95943"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1179],\n",
              "        [ 0.0351],\n",
              "        [ 1.5496],\n",
              "        ...,\n",
              "        [ 0.4535],\n",
              "        [ 1.5710],\n",
              "        [-1.7272]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgouQHdHsSej"
      },
      "source": [
        "# class Neural(nn.Module):\n",
        "#   def __init__(self,input_size,output_size):\n",
        "#     super().__init__()\n",
        "#     self.linear1=nn.Linear(input_size,output_size)\n",
        "#     # self.linear2=nn.Linear(output_size,1)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     print(\"1: \",x)\n",
        "#     x=nn.ReLU(self.linear1(x))\n",
        "#     # print(\"2: \",x)\n",
        "#     # x=nn.ReLU(self.linear2(x))\n",
        "#     return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyxl9YkXusKn"
      },
      "source": [
        "# model=Neural(1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1CW_X98uw2L"
      },
      "source": [
        "# print(model(x_torch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya_Smai_u-Nb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRjxZYcvUJn"
      },
      "source": [
        "class Neural2(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(input_size,32)\n",
        "    self.linear2=nn.Linear(32,64)\n",
        "    self.linear3=nn.Linear(64,128)\n",
        "    self.linear4=nn.Linear(128,1)\n",
        " \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=F.relu(self.linear1(x))\n",
        "    x=F.relu(self.linear2(x))\n",
        "    x=F.relu(self.linear3(x))\n",
        "    # x=F.log_softmax(self.linear4(x))\n",
        "    x=self.linear4(x)\n",
        "    # x=nn.Softmax(dim=1)(x)\n",
        "    \n",
        "   \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQEFERLCUNWZ"
      },
      "source": [
        "class Neural2(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(input_size,32)\n",
        "    # self.linear2=nn.Linear(32,64)\n",
        "    # self.linear3=nn.Linear(64,128)\n",
        "    self.linear4=nn.Linear(32,1)\n",
        " \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.linear1(x)\n",
        "    # x=self.linear2(x)\n",
        "    # x=self.linear3(x)\n",
        "    # x=F.log_softmax(self.linear4(x))\n",
        "    x=self.linear4(x)\n",
        "    # x=nn.Softmax(dim=1)(x)\n",
        "    \n",
        "   \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRKkTG-vXoC"
      },
      "source": [
        "model=Neural2(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJiXTamvZba",
        "outputId": "71f94aaa-c86d-4d7a-c8a2-bac5e93290b4"
      },
      "source": [
        "for i in range(100):\n",
        "  print(model(x_torch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ -67.9014],\n",
            "        [ -74.3289],\n",
            "        [-138.0253],\n",
            "        ...,\n",
            "        [ -91.9322],\n",
            "        [-138.9518],\n",
            "        [  -0.2095]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wATIdS1vew9"
      },
      "source": [
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znKapNlpKrRw"
      },
      "source": [
        "def accuracy(predictions, labels):\n",
        "    classes = torch.argmax(predictions, dim=1)\n",
        "    return torch.mean((classes == labels).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXoQRgXyKrUX"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46CKUFmuHjYH",
        "outputId": "5cd4ee01-6274-429f-ce6e-1cb34fe8c613"
      },
      "source": [
        "model.train()\n",
        "total=0\n",
        "\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  output=model(X_train)\n",
        "  loss=criterion(output,Y_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "  print('epoch {}, loss {}'.format(i, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 0.0038823888171464205\n",
            "epoch 1, loss 0.0036713494919240475\n",
            "epoch 2, loss 0.0034718841779977083\n",
            "epoch 3, loss 0.0032833556178957224\n",
            "epoch 4, loss 0.0031051584519445896\n",
            "epoch 5, loss 0.002936720382422209\n",
            "epoch 6, loss 0.002777507295832038\n",
            "epoch 7, loss 0.0026270048692822456\n",
            "epoch 8, loss 0.002484731376171112\n",
            "epoch 9, loss 0.0023502414114773273\n",
            "epoch 10, loss 0.0022230963222682476\n",
            "epoch 11, loss 0.002102893777191639\n",
            "epoch 12, loss 0.0019892575219273567\n",
            "epoch 13, loss 0.0018818187527358532\n",
            "epoch 14, loss 0.0017802410293370485\n",
            "epoch 15, loss 0.0016842004843056202\n",
            "epoch 16, loss 0.0015933912945911288\n",
            "epoch 17, loss 0.0015075274277478456\n",
            "epoch 18, loss 0.0014263354241847992\n",
            "epoch 19, loss 0.0013495605671778321\n",
            "epoch 20, loss 0.0012769604800269008\n",
            "epoch 21, loss 0.0012083075707778335\n",
            "epoch 22, loss 0.00114338134881109\n",
            "epoch 23, loss 0.0010819791350513697\n",
            "epoch 24, loss 0.0010239091934636235\n",
            "epoch 25, loss 0.0009689885773696005\n",
            "epoch 26, loss 0.000917044177185744\n",
            "epoch 27, loss 0.0008679128368385136\n",
            "epoch 28, loss 0.0008214409463107586\n",
            "epoch 29, loss 0.0007774844416417181\n",
            "epoch 30, loss 0.0007359036244452\n",
            "epoch 31, loss 0.0006965717184357345\n",
            "epoch 32, loss 0.0006593636935576797\n",
            "epoch 33, loss 0.0006241650553420186\n",
            "epoch 34, loss 0.000590864394325763\n",
            "epoch 35, loss 0.000559360662009567\n",
            "epoch 36, loss 0.0005295544397085905\n",
            "epoch 37, loss 0.0005013530026189983\n",
            "epoch 38, loss 0.00047467066906392574\n",
            "epoch 39, loss 0.0004494228633120656\n",
            "epoch 40, loss 0.00042553487583063543\n",
            "epoch 41, loss 0.0004029278934467584\n",
            "epoch 42, loss 0.0003815367235802114\n",
            "epoch 43, loss 0.0003612940781749785\n",
            "epoch 44, loss 0.00034213787876069546\n",
            "epoch 45, loss 0.000324008462484926\n",
            "epoch 46, loss 0.00030684928060509264\n",
            "epoch 47, loss 0.000290610158117488\n",
            "epoch 48, loss 0.00027523955213837326\n",
            "epoch 49, loss 0.0002606908674351871\n",
            "epoch 50, loss 0.00024692059378139675\n",
            "epoch 51, loss 0.00023388522095046937\n",
            "epoch 52, loss 0.00022154624457471073\n",
            "epoch 53, loss 0.0002098663244396448\n",
            "epoch 54, loss 0.00019880880427081138\n",
            "epoch 55, loss 0.00018834031652659178\n",
            "epoch 56, loss 0.00017842942907009274\n",
            "epoch 57, loss 0.00016904616495594382\n",
            "epoch 58, loss 0.00016016252629924566\n",
            "epoch 59, loss 0.0001517516648164019\n",
            "epoch 60, loss 0.00014378681953530759\n",
            "epoch 61, loss 0.00013624511484522372\n",
            "epoch 62, loss 0.00012910348596051335\n",
            "epoch 63, loss 0.00012234135647304356\n",
            "epoch 64, loss 0.00011593670933507383\n",
            "epoch 65, loss 0.00010987168934661895\n",
            "epoch 66, loss 0.00010412740084575489\n",
            "epoch 67, loss 9.868681081570685e-05\n",
            "epoch 68, loss 9.353430505143479e-05\n",
            "epoch 69, loss 8.865405834512785e-05\n",
            "epoch 70, loss 8.403184619965032e-05\n",
            "epoch 71, loss 7.965348777361214e-05\n",
            "epoch 72, loss 7.550620648544282e-05\n",
            "epoch 73, loss 7.157695654314011e-05\n",
            "epoch 74, loss 6.785517325624824e-05\n",
            "epoch 75, loss 6.432940426748246e-05\n",
            "epoch 76, loss 6.098905942053534e-05\n",
            "epoch 77, loss 5.782400694442913e-05\n",
            "epoch 78, loss 5.4825104598421603e-05\n",
            "epoch 79, loss 5.1983319281134754e-05\n",
            "epoch 80, loss 4.92909639433492e-05\n",
            "epoch 81, loss 4.6739656681893393e-05\n",
            "epoch 82, loss 4.432243076735176e-05\n",
            "epoch 83, loss 4.203144635539502e-05\n",
            "epoch 84, loss 3.986072260886431e-05\n",
            "epoch 85, loss 3.7803456507390365e-05\n",
            "epoch 86, loss 3.58534962288104e-05\n",
            "epoch 87, loss 3.4005584893748164e-05\n",
            "epoch 88, loss 3.2254072721116245e-05\n",
            "epoch 89, loss 3.0594052077503875e-05\n",
            "epoch 90, loss 2.902045707742218e-05\n",
            "epoch 91, loss 2.752894397417549e-05\n",
            "epoch 92, loss 2.6115079890587367e-05\n",
            "epoch 93, loss 2.4774744815658778e-05\n",
            "epoch 94, loss 2.350405338802375e-05\n",
            "epoch 95, loss 2.229963502031751e-05\n",
            "epoch 96, loss 2.1157573428354226e-05\n",
            "epoch 97, loss 2.0074701751582325e-05\n",
            "epoch 98, loss 1.9048156900680624e-05\n",
            "epoch 99, loss 1.8074702893500216e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sQ-YPbTLHcy",
        "outputId": "57db02af-2fbb-4952-f58c-767133c1884b"
      },
      "source": [
        "\n",
        "model.train()\n",
        "\n",
        "for i in range(100):\n",
        "  train_running_loss=0\n",
        "  train_running_acc=0\n",
        "  loss_plt=[]\n",
        "  acc_plt=[]\n",
        "  for inputs,target in trainloader:\n",
        "    # inputs,target=data[0],data[1]\n",
        "    optimizer.zero_grad()\n",
        "    output=model(inputs)\n",
        "    loss=criterion(output,target.unsqueeze(0))\n",
        "    acc=binary_acc(output,target.unsqueeze(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_running_loss+=loss.item()\n",
        "    train_running_acc+=acc.item()\n",
        "\n",
        "    acc_plt.append(train_running_acc)\n",
        "    loss_plt.append(train_running_loss)\n",
        "  print(f'Epoch {i+0:03}: | Loss: {train_running_loss/len(trainloader):.5f} | Acc: {train_running_acc/len(trainloader):.3f}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 3, 1])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000: | Loss: 0.00132 | Acc: 0.187\n",
            "Epoch 001: | Loss: 0.00009 | Acc: 0.187\n",
            "Epoch 002: | Loss: 0.00005 | Acc: 0.187\n",
            "Epoch 003: | Loss: 0.00003 | Acc: 0.187\n",
            "Epoch 004: | Loss: 0.00001 | Acc: 0.187\n",
            "Epoch 005: | Loss: 0.00001 | Acc: 0.187\n",
            "Epoch 006: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 007: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 008: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 009: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 010: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 011: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 012: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 013: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 014: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 015: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 016: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 017: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 018: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 019: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 020: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 021: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 022: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 023: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 024: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 025: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 026: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 027: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 028: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 029: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 030: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 031: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 032: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 033: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 034: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 035: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 036: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 037: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 038: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 039: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 040: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 041: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 042: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 043: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 044: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 045: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 046: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 047: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 048: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 049: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 050: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 051: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 052: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 053: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 054: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 055: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 056: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 057: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 058: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 059: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 060: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 061: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 062: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 063: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 064: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 065: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 066: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 067: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 068: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 069: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 070: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 071: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 072: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 073: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 074: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 075: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 076: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 077: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 078: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 079: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 080: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 081: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 082: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 083: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 084: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 085: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 086: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 087: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 088: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 089: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 090: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 091: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 092: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 093: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 094: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 095: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 096: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 097: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 098: | Loss: 0.00000 | Acc: 0.187\n",
            "Epoch 099: | Loss: 0.00000 | Acc: 0.187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Xt-RoX_Kdo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozp3egLjUl0j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9ATtECMvUl4J",
        "outputId": "243321b5-90db-4331-fbcc-2b7e8aad8d62"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Celsius</th>\n",
              "      <th>Fahrenheit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1169</td>\n",
              "      <td>2136.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1280</td>\n",
              "      <td>2336.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2380</td>\n",
              "      <td>4316.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1508</td>\n",
              "      <td>2746.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>3659.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Celsius  Fahrenheit\n",
              "0     1169      2136.2\n",
              "1     1280      2336.0\n",
              "2     2380      4316.0\n",
              "3     1508      2746.4\n",
              "4     2015      3659.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TiE3qF5IB1D"
      },
      "source": [
        "real_val_t=min_max.inverse_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYED-UrZG336",
        "outputId": "0a6953c4-6b3f-4f9a-a368-302021d8c0c3"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1180],\n",
              "        [ 0.0348],\n",
              "        [ 1.5493],\n",
              "        ...,\n",
              "        [ 0.4533],\n",
              "        [ 1.5713],\n",
              "        [-1.7275]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbI4-LvwIJPk",
        "outputId": "d7a02799-2088-4970-ec33-6ef4cfbce8bc"
      },
      "source": [
        "real_val_t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.16900000e+03],\n",
              "       [ 1.28000000e+03],\n",
              "       [ 2.38000003e+03],\n",
              "       ...,\n",
              "       [ 1.58400000e+03],\n",
              "       [ 2.39599998e+03],\n",
              "       [-3.06101763e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0zHKMujVZ1Z",
        "outputId": "30533dbf-46e3-4e1c-997b-fd54f042f751"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1180],\n",
              "        [ 0.0348],\n",
              "        [ 1.5493],\n",
              "        ...,\n",
              "        [ 0.4533],\n",
              "        [ 1.5713],\n",
              "        [-1.7275]])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9zctmb3_Kg3",
        "outputId": "a0c966f1-77b3-44a9-c450-43fafa53490f"
      },
      "source": [
        "y_val_t=min_max2.inverse_transform(Y_train)\n",
        "y_val_t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2136.20000193],\n",
              "       [2335.99999811],\n",
              "       [4316.0000554 ],\n",
              "       ...,\n",
              "       [2883.19999467],\n",
              "       [4344.79996499],\n",
              "       [  31.9999449 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHzFXvuiHjed"
      },
      "source": [
        "predicted = model(X_train).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oqEM-Z_Sfre",
        "outputId": "1dfe1cf1-a86f-406e-84bf-5b394a7675f2"
      },
      "source": [
        "predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.11619096],\n",
              "       [ 0.03603999],\n",
              "       [ 1.5446349 ],\n",
              "       ...,\n",
              "       [ 0.4529608 ],\n",
              "       [ 1.566578  ],\n",
              "       [-1.7194158 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY7EOqL5H3eT"
      },
      "source": [
        "all_predict=min_max2.inverse_transform(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjw5cVkE0VBD",
        "outputId": "296e9beb-3bc3-44d9-b5b1-5c98331f4ddc"
      },
      "source": [
        "all_predict=min_max2.inverse_transform(predicted)\n",
        "len(all_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PYXeIb4VUxm0",
        "outputId": "fb798f80-5fc8-4851-cdba-f29d8ec34a4a"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Celsius</th>\n",
              "      <th>Fahrenheit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1169</td>\n",
              "      <td>2136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1280</td>\n",
              "      <td>2336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2380</td>\n",
              "      <td>4316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1508</td>\n",
              "      <td>2746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>3659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Celsius  Fahrenheit\n",
              "0     1169        2136\n",
              "1     1280        2336\n",
              "2     2380        4316\n",
              "3     1508        2746\n",
              "4     2015        3659"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIx9Et9fUxp8"
      },
      "source": [
        "predict=pd.DataFrame(data={\"Celsius\":torch.tensor(data[\"Celsius\"]).flatten(),\"Fahrenheit\":torch.tensor(data[\"Fahrenheit\"]).flatten(),\"Predict\":all_predict.flatten()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "yupDZWJo4Q0R",
        "outputId": "9edfe52e-76c8-43dc-d9ec-8e9b3e384eeb"
      },
      "source": [
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Celsius</th>\n",
              "      <th>Fahrenheit</th>\n",
              "      <th>Predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1169</td>\n",
              "      <td>2136</td>\n",
              "      <td>2138.221191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1280</td>\n",
              "      <td>2336</td>\n",
              "      <td>2337.242920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2380</td>\n",
              "      <td>4316</td>\n",
              "      <td>4309.531250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1508</td>\n",
              "      <td>2746</td>\n",
              "      <td>2746.044678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>3659</td>\n",
              "      <td>3655.090088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>833</td>\n",
              "      <td>1531</td>\n",
              "      <td>1535.776855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>757</td>\n",
              "      <td>1394</td>\n",
              "      <td>1399.509766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>1584</td>\n",
              "      <td>2883</td>\n",
              "      <td>2882.311768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>2396</td>\n",
              "      <td>4344</td>\n",
              "      <td>4338.218750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>42.216991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Celsius  Fahrenheit      Predict\n",
              "0         1169        2136  2138.221191\n",
              "1         1280        2336  2337.242920\n",
              "2         2380        4316  4309.531250\n",
              "3         1508        2746  2746.044678\n",
              "4         2015        3659  3655.090088\n",
              "...        ...         ...          ...\n",
              "15995      833        1531  1535.776855\n",
              "15996      757        1394  1399.509766\n",
              "15997     1584        2883  2882.311768\n",
              "15998     2396        4344  4338.218750\n",
              "15999        0          32    42.216991\n",
              "\n",
              "[16000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yasGUNLV4Q2T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y66a0UNN4Q5e"
      },
      "source": [
        "x=torch.Tensor([10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa5XBf844Q8B",
        "outputId": "04fe8919-44d0-40e3-a91d-980524419a68"
      },
      "source": [
        "a=model(x).data.numpy()\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.380795], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StWc-YIl7ZMC",
        "outputId": "604a42b9-e3f5-4d96-d18f-87c531f285b8"
      },
      "source": [
        "min_max2.inverse_transform(a.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14554.271]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhVjUfQh7ZPG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgw8lWI27ZRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZR_knf2Uxsg"
      },
      "source": [
        "a=data[\"Celsius\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_sl3IUg1U7Z",
        "outputId": "6f709390-00c1-405b-84a5-ce051f0dcf74"
      },
      "source": [
        "all_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2223.1338 ],\n",
              "       [2403.1858 ],\n",
              "       [4187.4834 ],\n",
              "       ...,\n",
              "       [2896.3008 ],\n",
              "       [4213.437  ],\n",
              "       [ 326.91174]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOttMt9I1U-V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGabrM1Z1VDN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VCBymOV1VIQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "sQRbz1nLSsUc",
        "outputId": "96a3a018-e6de-4ac9-d661-595f8a71fdb2"
      },
      "source": [
        "plt.scatter(X_train.detach().numpy()[:100] , Y_train.detach().numpy()[:100])\n",
        "plt.plot(X_train.detach().numpy()[:100] , predicted[:100] , \"red\")\n",
        "plt.xlabel(\"Celcius\")\n",
        "plt.ylabel(\"Farenhite\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e72S7WPFIAmb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqsPHu7iIApT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeWOJBQoS0g7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVS81TCTFNY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctxj-yDoTFP7",
        "outputId": "9e542669-326f-445a-9568-b1bbc9830b97"
      },
      "source": [
        "total=0\n",
        "for i in range(500):\n",
        "  output=model(X_train)\n",
        "  loss=criterion(output,Y_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        " \n",
        "\n",
        "  print('epoch {}, loss {}'.format(i, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 0.05134893208742142\n",
            "epoch 1, loss 0.05115901678800583\n",
            "epoch 2, loss 0.05096929892897606\n",
            "epoch 3, loss 0.05077976733446121\n",
            "epoch 4, loss 0.05059041082859039\n",
            "epoch 5, loss 0.05040126293897629\n",
            "epoch 6, loss 0.050212327390909195\n",
            "epoch 7, loss 0.05002354457974434\n",
            "epoch 8, loss 0.04983493685722351\n",
            "epoch 9, loss 0.049646493047475815\n",
            "epoch 10, loss 0.04945821315050125\n",
            "epoch 11, loss 0.04927009716629982\n",
            "epoch 12, loss 0.04908212646842003\n",
            "epoch 13, loss 0.048894330859184265\n",
            "epoch 14, loss 0.04870672896504402\n",
            "epoch 15, loss 0.04851926863193512\n",
            "epoch 16, loss 0.048331983387470245\n",
            "epoch 17, loss 0.048144835978746414\n",
            "epoch 18, loss 0.047957856208086014\n",
            "epoch 19, loss 0.04777108505368233\n",
            "epoch 20, loss 0.047584448009729385\n",
            "epoch 21, loss 0.04739798605442047\n",
            "epoch 22, loss 0.04721170291304588\n",
            "epoch 23, loss 0.04702557250857353\n",
            "epoch 24, loss 0.04683960601687431\n",
            "epoch 25, loss 0.046653784811496735\n",
            "epoch 26, loss 0.04646812751889229\n",
            "epoch 27, loss 0.04628261551260948\n",
            "epoch 28, loss 0.046097271144390106\n",
            "epoch 29, loss 0.04591210186481476\n",
            "epoch 30, loss 0.04572710022330284\n",
            "epoch 31, loss 0.045542266219854355\n",
            "epoch 32, loss 0.0453576035797596\n",
            "epoch 33, loss 0.04517313092947006\n",
            "epoch 34, loss 0.04498882219195366\n",
            "epoch 35, loss 0.04480469599366188\n",
            "epoch 36, loss 0.044620756059885025\n",
            "epoch 37, loss 0.044436994940042496\n",
            "epoch 38, loss 0.04425341635942459\n",
            "epoch 39, loss 0.04407002031803131\n",
            "epoch 40, loss 0.043886806815862656\n",
            "epoch 41, loss 0.043703798204660416\n",
            "epoch 42, loss 0.0435209721326828\n",
            "epoch 43, loss 0.0433383472263813\n",
            "epoch 44, loss 0.04315590485930443\n",
            "epoch 45, loss 0.04297368600964546\n",
            "epoch 46, loss 0.04279164969921112\n",
            "epoch 47, loss 0.04260982573032379\n",
            "epoch 48, loss 0.04242821782827377\n",
            "epoch 49, loss 0.04224680736660957\n",
            "epoch 50, loss 0.042065612971782684\n",
            "epoch 51, loss 0.041884645819664\n",
            "epoch 52, loss 0.04170388728380203\n",
            "epoch 53, loss 0.041523370891809464\n",
            "epoch 54, loss 0.0413430891931057\n",
            "epoch 55, loss 0.04116304963827133\n",
            "epoch 56, loss 0.04098323732614517\n",
            "epoch 57, loss 0.04080365225672722\n",
            "epoch 58, loss 0.040624309331178665\n",
            "epoch 59, loss 0.04044521227478981\n",
            "epoch 60, loss 0.04026634991168976\n",
            "epoch 61, loss 0.04008778557181358\n",
            "epoch 62, loss 0.039909478276968\n",
            "epoch 63, loss 0.039731431752443314\n",
            "epoch 64, loss 0.03955363109707832\n",
            "epoch 65, loss 0.039376113563776016\n",
            "epoch 66, loss 0.0391988530755043\n",
            "epoch 67, loss 0.03902183473110199\n",
            "epoch 68, loss 0.038845088332891464\n",
            "epoch 69, loss 0.03866859897971153\n",
            "epoch 70, loss 0.038492351770401\n",
            "epoch 71, loss 0.038316402584314346\n",
            "epoch 72, loss 0.03814072906970978\n",
            "epoch 73, loss 0.037965305149555206\n",
            "epoch 74, loss 0.037790145725011826\n",
            "epoch 75, loss 0.03761528059840202\n",
            "epoch 76, loss 0.0374407097697258\n",
            "epoch 77, loss 0.037266410887241364\n",
            "epoch 78, loss 0.03709240257740021\n",
            "epoch 79, loss 0.036918677389621735\n",
            "epoch 80, loss 0.036745235323905945\n",
            "epoch 81, loss 0.036572109907865524\n",
            "epoch 82, loss 0.03639926761388779\n",
            "epoch 83, loss 0.0362267829477787\n",
            "epoch 84, loss 0.03605465218424797\n",
            "epoch 85, loss 0.035882920026779175\n",
            "epoch 86, loss 0.03571159020066261\n",
            "epoch 87, loss 0.03554068133234978\n",
            "epoch 88, loss 0.035370174795389175\n",
            "epoch 89, loss 0.035200085490942\n",
            "epoch 90, loss 0.03503037244081497\n",
            "epoch 91, loss 0.03486109524965286\n",
            "epoch 92, loss 0.034692201763391495\n",
            "epoch 93, loss 0.03452379256486893\n",
            "epoch 94, loss 0.03435583785176277\n",
            "epoch 95, loss 0.03418835625052452\n",
            "epoch 96, loss 0.034021373838186264\n",
            "epoch 97, loss 0.033854834735393524\n",
            "epoch 98, loss 0.033688735216856\n",
            "epoch 99, loss 0.033523090183734894\n",
            "epoch 100, loss 0.033357877284288406\n",
            "epoch 101, loss 0.03319310396909714\n",
            "epoch 102, loss 0.03302876278758049\n",
            "epoch 103, loss 0.03286483511328697\n",
            "epoch 104, loss 0.03270130977034569\n",
            "epoch 105, loss 0.03253818675875664\n",
            "epoch 106, loss 0.032375454902648926\n",
            "epoch 107, loss 0.03221311792731285\n",
            "epoch 108, loss 0.032051168382167816\n",
            "epoch 109, loss 0.031889624893665314\n",
            "epoch 110, loss 0.031728461384773254\n",
            "epoch 111, loss 0.03156770393252373\n",
            "epoch 112, loss 0.031407326459884644\n",
            "epoch 113, loss 0.0312473326921463\n",
            "epoch 114, loss 0.031087763607501984\n",
            "epoch 115, loss 0.03092854842543602\n",
            "epoch 116, loss 0.030769696459174156\n",
            "epoch 117, loss 0.0306111928075552\n",
            "epoch 118, loss 0.030453059822320938\n",
            "epoch 119, loss 0.03029528260231018\n",
            "epoch 120, loss 0.030137842521071434\n",
            "epoch 121, loss 0.029980745166540146\n",
            "epoch 122, loss 0.029823968186974525\n",
            "epoch 123, loss 0.029667535796761513\n",
            "epoch 124, loss 0.029511434957385063\n",
            "epoch 125, loss 0.02935568429529667\n",
            "epoch 126, loss 0.029200274497270584\n",
            "epoch 127, loss 0.029045185074210167\n",
            "epoch 128, loss 0.02889043278992176\n",
            "epoch 129, loss 0.028736019507050514\n",
            "epoch 130, loss 0.02858193963766098\n",
            "epoch 131, loss 0.02842818759381771\n",
            "epoch 132, loss 0.02827477641403675\n",
            "epoch 133, loss 0.028121689334511757\n",
            "epoch 134, loss 0.027968937531113625\n",
            "epoch 135, loss 0.02781653217971325\n",
            "epoch 136, loss 0.027664456516504288\n",
            "epoch 137, loss 0.027512717992067337\n",
            "epoch 138, loss 0.027361318469047546\n",
            "epoch 139, loss 0.027210257947444916\n",
            "epoch 140, loss 0.027059556916356087\n",
            "epoch 141, loss 0.026909194886684418\n",
            "epoch 142, loss 0.02675916999578476\n",
            "epoch 143, loss 0.02660949155688286\n",
            "epoch 144, loss 0.02646016515791416\n",
            "epoch 145, loss 0.026311174035072327\n",
            "epoch 146, loss 0.02616254612803459\n",
            "epoch 147, loss 0.026014244183897972\n",
            "epoch 148, loss 0.025866298004984856\n",
            "epoch 149, loss 0.025718698278069496\n",
            "epoch 150, loss 0.025571448728442192\n",
            "epoch 151, loss 0.02542455680668354\n",
            "epoch 152, loss 0.02527802437543869\n",
            "epoch 153, loss 0.025131843984127045\n",
            "epoch 154, loss 0.024986015632748604\n",
            "epoch 155, loss 0.02484053745865822\n",
            "epoch 156, loss 0.024695415049791336\n",
            "epoch 157, loss 0.02455066330730915\n",
            "epoch 158, loss 0.02440626174211502\n",
            "epoch 159, loss 0.02426222153007984\n",
            "epoch 160, loss 0.02411855384707451\n",
            "epoch 161, loss 0.023975247517228127\n",
            "epoch 162, loss 0.023832326754927635\n",
            "epoch 163, loss 0.02368977479636669\n",
            "epoch 164, loss 0.02354760840535164\n",
            "epoch 165, loss 0.023405808955430984\n",
            "epoch 166, loss 0.02326440066099167\n",
            "epoch 167, loss 0.02312336675822735\n",
            "epoch 168, loss 0.022982731461524963\n",
            "epoch 169, loss 0.022842470556497574\n",
            "epoch 170, loss 0.022702600806951523\n",
            "epoch 171, loss 0.022563118487596512\n",
            "epoch 172, loss 0.02242402732372284\n",
            "epoch 173, loss 0.022285334765911102\n",
            "epoch 174, loss 0.022147029638290405\n",
            "epoch 175, loss 0.02200913242995739\n",
            "epoch 176, loss 0.02187163755297661\n",
            "epoch 177, loss 0.021734533831477165\n",
            "epoch 178, loss 0.021597836166620255\n",
            "epoch 179, loss 0.021461548283696175\n",
            "epoch 180, loss 0.021325677633285522\n",
            "epoch 181, loss 0.021190207451581955\n",
            "epoch 182, loss 0.021055154502391815\n",
            "epoch 183, loss 0.020920520648360252\n",
            "epoch 184, loss 0.02078629471361637\n",
            "epoch 185, loss 0.020652491599321365\n",
            "epoch 186, loss 0.020519111305475235\n",
            "epoch 187, loss 0.02038615755736828\n",
            "epoch 188, loss 0.02025362104177475\n",
            "epoch 189, loss 0.020121516659855843\n",
            "epoch 190, loss 0.01998983696103096\n",
            "epoch 191, loss 0.019858596846461296\n",
            "epoch 192, loss 0.01972779631614685\n",
            "epoch 193, loss 0.019597431644797325\n",
            "epoch 194, loss 0.01946752332150936\n",
            "epoch 195, loss 0.019338082522153854\n",
            "epoch 196, loss 0.019209127873182297\n",
            "epoch 197, loss 0.01908062770962715\n",
            "epoch 198, loss 0.01895259879529476\n",
            "epoch 199, loss 0.018825028091669083\n",
            "epoch 200, loss 0.018697934225201607\n",
            "epoch 201, loss 0.01857130602002144\n",
            "epoch 202, loss 0.018445145338773727\n",
            "epoch 203, loss 0.018319454044103622\n",
            "epoch 204, loss 0.01819423958659172\n",
            "epoch 205, loss 0.01806948520243168\n",
            "epoch 206, loss 0.01794521138072014\n",
            "epoch 207, loss 0.017821406945586205\n",
            "epoch 208, loss 0.01769808679819107\n",
            "epoch 209, loss 0.01757524535059929\n",
            "epoch 210, loss 0.017452877014875412\n",
            "epoch 211, loss 0.017331000417470932\n",
            "epoch 212, loss 0.017209596931934357\n",
            "epoch 213, loss 0.017088674008846283\n",
            "epoch 214, loss 0.01696823351085186\n",
            "epoch 215, loss 0.01684829406440258\n",
            "epoch 216, loss 0.016728900372982025\n",
            "epoch 217, loss 0.016610078513622284\n",
            "epoch 218, loss 0.01649184711277485\n",
            "epoch 219, loss 0.0163742508739233\n",
            "epoch 220, loss 0.01625724881887436\n",
            "epoch 221, loss 0.016140809282660484\n",
            "epoch 222, loss 0.016024932265281677\n",
            "epoch 223, loss 0.015909623354673386\n",
            "epoch 224, loss 0.015794886276125908\n",
            "epoch 225, loss 0.015680693089962006\n",
            "epoch 226, loss 0.015567053109407425\n",
            "epoch 227, loss 0.015453962609171867\n",
            "epoch 228, loss 0.015341412276029587\n",
            "epoch 229, loss 0.015229441225528717\n",
            "epoch 230, loss 0.015118054114282131\n",
            "epoch 231, loss 0.015007218345999718\n",
            "epoch 232, loss 0.014896953478455544\n",
            "epoch 233, loss 0.014787224121391773\n",
            "epoch 234, loss 0.014678037725389004\n",
            "epoch 235, loss 0.014569401741027832\n",
            "epoch 236, loss 0.014461326412856579\n",
            "epoch 237, loss 0.014353803358972073\n",
            "epoch 238, loss 0.01424684002995491\n",
            "epoch 239, loss 0.014140425249934196\n",
            "epoch 240, loss 0.014034549705684185\n",
            "epoch 241, loss 0.013929231092333794\n",
            "epoch 242, loss 0.013824446126818657\n",
            "epoch 243, loss 0.013720216229557991\n",
            "epoch 244, loss 0.013616523705422878\n",
            "epoch 245, loss 0.013513387180864811\n",
            "epoch 246, loss 0.013410788960754871\n",
            "epoch 247, loss 0.013308730907738209\n",
            "epoch 248, loss 0.01320720836520195\n",
            "epoch 249, loss 0.01310623250901699\n",
            "epoch 250, loss 0.013005798682570457\n",
            "epoch 251, loss 0.01290590688586235\n",
            "epoch 252, loss 0.012806542217731476\n",
            "epoch 253, loss 0.012707733549177647\n",
            "epoch 254, loss 0.012609472498297691\n",
            "epoch 255, loss 0.012511751614511013\n",
            "epoch 256, loss 0.012414566241204739\n",
            "epoch 257, loss 0.012317942455410957\n",
            "epoch 258, loss 0.012221838347613811\n",
            "epoch 259, loss 0.012126250192523003\n",
            "epoch 260, loss 0.01203121431171894\n",
            "epoch 261, loss 0.011936705559492111\n",
            "epoch 262, loss 0.011842722073197365\n",
            "epoch 263, loss 0.011749258264899254\n",
            "epoch 264, loss 0.011656323447823524\n",
            "epoch 265, loss 0.011563925072550774\n",
            "epoch 266, loss 0.01147205289453268\n",
            "epoch 267, loss 0.011380700394511223\n",
            "epoch 268, loss 0.011289872229099274\n",
            "epoch 269, loss 0.011199570260941982\n",
            "epoch 270, loss 0.011109781451523304\n",
            "epoch 271, loss 0.011020501144230366\n",
            "epoch 272, loss 0.010931741446256638\n",
            "epoch 273, loss 0.010843497700989246\n",
            "epoch 274, loss 0.010755792260169983\n",
            "epoch 275, loss 0.010668603703379631\n",
            "epoch 276, loss 0.010581934824585915\n",
            "epoch 277, loss 0.010495796799659729\n",
            "epoch 278, loss 0.010410172864794731\n",
            "epoch 279, loss 0.010325073264539242\n",
            "epoch 280, loss 0.010240479372441769\n",
            "epoch 281, loss 0.010156402364373207\n",
            "epoch 282, loss 0.010072828270494938\n",
            "epoch 283, loss 0.009989771991968155\n",
            "epoch 284, loss 0.00990721583366394\n",
            "epoch 285, loss 0.009825173765420914\n",
            "epoch 286, loss 0.00974363274872303\n",
            "epoch 287, loss 0.009662594646215439\n",
            "epoch 288, loss 0.00958205945789814\n",
            "epoch 289, loss 0.009502017870545387\n",
            "epoch 290, loss 0.009422477334737778\n",
            "epoch 291, loss 0.009343430399894714\n",
            "epoch 292, loss 0.009264875203371048\n",
            "epoch 293, loss 0.009186815470457077\n",
            "epoch 294, loss 0.009109253995120525\n",
            "epoch 295, loss 0.00903218612074852\n",
            "epoch 296, loss 0.008955619297921658\n",
            "epoch 297, loss 0.008879544213414192\n",
            "epoch 298, loss 0.008803953416645527\n",
            "epoch 299, loss 0.008728856220841408\n",
            "epoch 300, loss 0.008654247969388962\n",
            "epoch 301, loss 0.008580133318901062\n",
            "epoch 302, loss 0.008506505750119686\n",
            "epoch 303, loss 0.008433368988335133\n",
            "epoch 304, loss 0.008360711857676506\n",
            "epoch 305, loss 0.008288534358143806\n",
            "epoch 306, loss 0.00821684766560793\n",
            "epoch 307, loss 0.008145632222294807\n",
            "epoch 308, loss 0.008074897341430187\n",
            "epoch 309, loss 0.008004633709788322\n",
            "epoch 310, loss 0.007934856228530407\n",
            "epoch 311, loss 0.007865549996495247\n",
            "epoch 312, loss 0.007796711754053831\n",
            "epoch 313, loss 0.007728346157819033\n",
            "epoch 314, loss 0.007660453673452139\n",
            "epoch 315, loss 0.007593019865453243\n",
            "epoch 316, loss 0.007526055909693241\n",
            "epoch 317, loss 0.0074595557525753975\n",
            "epoch 318, loss 0.007393514271825552\n",
            "epoch 319, loss 0.007327933330088854\n",
            "epoch 320, loss 0.007262812461704016\n",
            "epoch 321, loss 0.0071981498040258884\n",
            "epoch 322, loss 0.007133953273296356\n",
            "epoch 323, loss 0.007070210296660662\n",
            "epoch 324, loss 0.007006920874118805\n",
            "epoch 325, loss 0.006944085471332073\n",
            "epoch 326, loss 0.0068816919811069965\n",
            "epoch 327, loss 0.00681975157931447\n",
            "epoch 328, loss 0.0067582568153738976\n",
            "epoch 329, loss 0.006697206292301416\n",
            "epoch 330, loss 0.0066366009414196014\n",
            "epoch 331, loss 0.006576435174793005\n",
            "epoch 332, loss 0.006516709458082914\n",
            "epoch 333, loss 0.0064574251882731915\n",
            "epoch 334, loss 0.0063985674642026424\n",
            "epoch 335, loss 0.006340154446661472\n",
            "epoch 336, loss 0.006282167509198189\n",
            "epoch 337, loss 0.006224615965038538\n",
            "epoch 338, loss 0.006167492363601923\n",
            "epoch 339, loss 0.006110798567533493\n",
            "epoch 340, loss 0.0060545289888978004\n",
            "epoch 341, loss 0.00599868968129158\n",
            "epoch 342, loss 0.005943275522440672\n",
            "epoch 343, loss 0.0058882818557322025\n",
            "epoch 344, loss 0.0058337049558758736\n",
            "epoch 345, loss 0.00577954389154911\n",
            "epoch 346, loss 0.005725795868784189\n",
            "epoch 347, loss 0.005672462750226259\n",
            "epoch 348, loss 0.005619540344923735\n",
            "epoch 349, loss 0.005567025393247604\n",
            "epoch 350, loss 0.005514920689165592\n",
            "epoch 351, loss 0.005463219713419676\n",
            "epoch 352, loss 0.005411925725638866\n",
            "epoch 353, loss 0.005361036863178015\n",
            "epoch 354, loss 0.005310547538101673\n",
            "epoch 355, loss 0.005260461010038853\n",
            "epoch 356, loss 0.0052107698284089565\n",
            "epoch 357, loss 0.0051614753901958466\n",
            "epoch 358, loss 0.005112568847835064\n",
            "epoch 359, loss 0.005064054392278194\n",
            "epoch 360, loss 0.005015928763896227\n",
            "epoch 361, loss 0.004968191962689161\n",
            "epoch 362, loss 0.0049208407290279865\n",
            "epoch 363, loss 0.004873870871961117\n",
            "epoch 364, loss 0.0048272861167788506\n",
            "epoch 365, loss 0.004781079012900591\n",
            "epoch 366, loss 0.0047352509573102\n",
            "epoch 367, loss 0.004689796827733517\n",
            "epoch 368, loss 0.004644712433218956\n",
            "epoch 369, loss 0.004600003361701965\n",
            "epoch 370, loss 0.004555661231279373\n",
            "epoch 371, loss 0.004511688370257616\n",
            "epoch 372, loss 0.004468079656362534\n",
            "epoch 373, loss 0.004424836486577988\n",
            "epoch 374, loss 0.004381953272968531\n",
            "epoch 375, loss 0.004339433275163174\n",
            "epoch 376, loss 0.004297267645597458\n",
            "epoch 377, loss 0.0042554596439003944\n",
            "epoch 378, loss 0.004214003682136536\n",
            "epoch 379, loss 0.004172903019934893\n",
            "epoch 380, loss 0.004132147412747145\n",
            "epoch 381, loss 0.004091743379831314\n",
            "epoch 382, loss 0.004051684867590666\n",
            "epoch 383, loss 0.004011968616396189\n",
            "epoch 384, loss 0.003972592297941446\n",
            "epoch 385, loss 0.003933555446565151\n",
            "epoch 386, loss 0.00389485782943666\n",
            "epoch 387, loss 0.003856494789943099\n",
            "epoch 388, loss 0.003818465629592538\n",
            "epoch 389, loss 0.0037807710468769073\n",
            "epoch 390, loss 0.0037434024270623922\n",
            "epoch 391, loss 0.00370636279694736\n",
            "epoch 392, loss 0.003669650759547949\n",
            "epoch 393, loss 0.0036332577001303434\n",
            "epoch 394, loss 0.003597188973799348\n",
            "epoch 395, loss 0.0035614389926195145\n",
            "epoch 396, loss 0.0035260042641311884\n",
            "epoch 397, loss 0.0034908864181488752\n",
            "epoch 398, loss 0.003456083359196782\n",
            "epoch 399, loss 0.003421592293307185\n",
            "epoch 400, loss 0.0033874076325446367\n",
            "epoch 401, loss 0.00335353659465909\n",
            "epoch 402, loss 0.003319965908303857\n",
            "epoch 403, loss 0.0032866992987692356\n",
            "epoch 404, loss 0.0032537360675632954\n",
            "epoch 405, loss 0.0032210692297667265\n",
            "epoch 406, loss 0.0031886999495327473\n",
            "epoch 407, loss 0.003156626597046852\n",
            "epoch 408, loss 0.003124844515696168\n",
            "epoch 409, loss 0.003093356266617775\n",
            "epoch 410, loss 0.003062156029045582\n",
            "epoch 411, loss 0.0030312419403344393\n",
            "epoch 412, loss 0.0030006146989762783\n",
            "epoch 413, loss 0.002970268949866295\n",
            "epoch 414, loss 0.0029402044601738453\n",
            "epoch 415, loss 0.0029104184359312057\n",
            "epoch 416, loss 0.0028809132054448128\n",
            "epoch 417, loss 0.002851680153980851\n",
            "epoch 418, loss 0.0028227230068296194\n",
            "epoch 419, loss 0.002794034546241164\n",
            "epoch 420, loss 0.0027656175661832094\n",
            "epoch 421, loss 0.00273746601305902\n",
            "epoch 422, loss 0.0027095815166831017\n",
            "epoch 423, loss 0.0026819612830877304\n",
            "epoch 424, loss 0.002654600888490677\n",
            "epoch 425, loss 0.0026275001000612974\n",
            "epoch 426, loss 0.0026006600819528103\n",
            "epoch 427, loss 0.002574074314907193\n",
            "epoch 428, loss 0.002547742798924446\n",
            "epoch 429, loss 0.00252166367135942\n",
            "epoch 430, loss 0.002495835768058896\n",
            "epoch 431, loss 0.0024702539667487144\n",
            "epoch 432, loss 0.002444921061396599\n",
            "epoch 433, loss 0.002419831696897745\n",
            "epoch 434, loss 0.002394985407590866\n",
            "epoch 435, loss 0.002370381262153387\n",
            "epoch 436, loss 0.002346016699448228\n",
            "epoch 437, loss 0.0023218884598463774\n",
            "epoch 438, loss 0.0022979946807026863\n",
            "epoch 439, loss 0.0022743376903235912\n",
            "epoch 440, loss 0.0022509105037897825\n",
            "epoch 441, loss 0.002227713819593191\n",
            "epoch 442, loss 0.002204745542258024\n",
            "epoch 443, loss 0.002182002877816558\n",
            "epoch 444, loss 0.002159485360607505\n",
            "epoch 445, loss 0.002137191826477647\n",
            "epoch 446, loss 0.002115117385983467\n",
            "epoch 447, loss 0.0020932622719556093\n",
            "epoch 448, loss 0.0020716257859021425\n",
            "epoch 449, loss 0.0020502042025327682\n",
            "epoch 450, loss 0.0020289986860007048\n",
            "epoch 451, loss 0.0020080041140317917\n",
            "epoch 452, loss 0.0019872214179486036\n",
            "epoch 453, loss 0.001966646406799555\n",
            "epoch 454, loss 0.0019462784985080361\n",
            "epoch 455, loss 0.0019261158304288983\n",
            "epoch 456, loss 0.0019061571219936013\n",
            "epoch 457, loss 0.0018863999284803867\n",
            "epoch 458, loss 0.0018668425036594272\n",
            "epoch 459, loss 0.001847481937147677\n",
            "epoch 460, loss 0.0018283217214047909\n",
            "epoch 461, loss 0.0018093553371727467\n",
            "epoch 462, loss 0.0017905815038830042\n",
            "epoch 463, loss 0.0017720015021041036\n",
            "epoch 464, loss 0.0017536100931465626\n",
            "epoch 465, loss 0.0017354082083329558\n",
            "epoch 466, loss 0.001717391307465732\n",
            "epoch 467, loss 0.0016995628830045462\n",
            "epoch 468, loss 0.0016819176962599158\n",
            "epoch 469, loss 0.0016644543502479792\n",
            "epoch 470, loss 0.0016471738927066326\n",
            "epoch 471, loss 0.0016300705028697848\n",
            "epoch 472, loss 0.0016131448792293668\n",
            "epoch 473, loss 0.001596396672539413\n",
            "epoch 474, loss 0.0015798215754330158\n",
            "epoch 475, loss 0.0015634195879101753\n",
            "epoch 476, loss 0.0015471904771402478\n",
            "epoch 477, loss 0.0015311309834942222\n",
            "epoch 478, loss 0.0015152383130043745\n",
            "epoch 479, loss 0.001499514444731176\n",
            "epoch 480, loss 0.0014839527430012822\n",
            "epoch 481, loss 0.0014685571659356356\n",
            "epoch 482, loss 0.001453322940506041\n",
            "epoch 483, loss 0.001438249833881855\n",
            "epoch 484, loss 0.0014233377296477556\n",
            "epoch 485, loss 0.0014085822040215135\n",
            "epoch 486, loss 0.0013939839554950595\n",
            "epoch 487, loss 0.0013795413542538881\n",
            "epoch 488, loss 0.00136525125708431\n",
            "epoch 489, loss 0.0013511145953088999\n",
            "epoch 490, loss 0.0013371278764680028\n",
            "epoch 491, loss 0.0013232913333922625\n",
            "epoch 492, loss 0.0013096029870212078\n",
            "epoch 493, loss 0.0012960598105564713\n",
            "epoch 494, loss 0.0012826628517359495\n",
            "epoch 495, loss 0.0012694110628217459\n",
            "epoch 496, loss 0.0012563006021082401\n",
            "epoch 497, loss 0.001243331702426076\n",
            "epoch 498, loss 0.001230502617545426\n",
            "epoch 499, loss 0.0012178116012364626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSFZN6WvTQYA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBHAG2UATQan"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5DqNkCTG3E"
      },
      "source": [
        "predicted = model(X_train).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "LIsSuejnTJ7c",
        "outputId": "bdfe5524-3336-44fd-9d5c-b07c87811d05"
      },
      "source": [
        "plt.scatter(X_train.detach().numpy()[:100] , Y_train.detach().numpy()[:100])\n",
        "plt.plot(X_train.detach().numpy()[:100] , predicted[:100] , \"red\")\n",
        "plt.xlabel(\"Celcius\")\n",
        "plt.ylabel(\"Farenhite\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wYWz-BbThJy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlhdwswTMj0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUpOtPeoTRLB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TVfa4w_TRNv",
        "outputId": "5a35439e-f7f8-4c9c-a934-58ec39ba78d5"
      },
      "source": [
        "total=0\n",
        "for i in range(1500):\n",
        "  output=model(X_train)\n",
        "  loss=criterion(output,Y_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        " \n",
        "  print('epoch {}, loss {}'.format(i, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 0.0012052578385919333\n",
            "epoch 1, loss 0.0011928414460271597\n",
            "epoch 2, loss 0.0011805600952357054\n",
            "epoch 3, loss 0.0011684101773425937\n",
            "epoch 4, loss 0.0011563933221623302\n",
            "epoch 5, loss 0.0011445073178038\n",
            "epoch 6, loss 0.001132751815021038\n",
            "epoch 7, loss 0.0011211242526769638\n",
            "epoch 8, loss 0.0011096236994490027\n",
            "epoch 9, loss 0.001098248758353293\n",
            "epoch 10, loss 0.0010869979159906507\n",
            "epoch 11, loss 0.0010758719872683287\n",
            "epoch 12, loss 0.0010648686438798904\n",
            "epoch 13, loss 0.0010539846261963248\n",
            "epoch 14, loss 0.0010432209819555283\n",
            "epoch 15, loss 0.0010325770126655698\n",
            "epoch 16, loss 0.0010220499243587255\n",
            "epoch 17, loss 0.0010116390185430646\n",
            "epoch 18, loss 0.0010013437131419778\n",
            "epoch 19, loss 0.0009911623783409595\n",
            "epoch 20, loss 0.0009810947813093662\n",
            "epoch 21, loss 0.0009711379534564912\n",
            "epoch 22, loss 0.0009612931753508747\n",
            "epoch 23, loss 0.0009515577694401145\n",
            "epoch 24, loss 0.000941929523833096\n",
            "epoch 25, loss 0.0009324098937213421\n",
            "epoch 26, loss 0.0009229968418367207\n",
            "epoch 27, loss 0.0009136878070421517\n",
            "epoch 28, loss 0.0009044835460372269\n",
            "epoch 29, loss 0.0008953824290074408\n",
            "epoch 30, loss 0.0008863831753842533\n",
            "epoch 31, loss 0.0008774856105446815\n",
            "epoch 32, loss 0.0008686876390129328\n",
            "epoch 33, loss 0.0008599886787123978\n",
            "epoch 34, loss 0.000851387798320502\n",
            "epoch 35, loss 0.0008428845903836191\n",
            "epoch 36, loss 0.000834476959425956\n",
            "epoch 37, loss 0.0008261639159172773\n",
            "epoch 38, loss 0.0008179445867426693\n",
            "epoch 39, loss 0.0008098189136944711\n",
            "epoch 40, loss 0.000801785965450108\n",
            "epoch 41, loss 0.0007938433554954827\n",
            "epoch 42, loss 0.0007859900360926986\n",
            "epoch 43, loss 0.0007782276370562613\n",
            "epoch 44, loss 0.0007705529569648206\n",
            "epoch 45, loss 0.0007629662286490202\n",
            "epoch 46, loss 0.000755465414840728\n",
            "epoch 47, loss 0.0007480496424250305\n",
            "epoch 48, loss 0.0007407195516861975\n",
            "epoch 49, loss 0.000733472581487149\n",
            "epoch 50, loss 0.0007263090810738504\n",
            "epoch 51, loss 0.0007192270713858306\n",
            "epoch 52, loss 0.0007122264942154288\n",
            "epoch 53, loss 0.0007053063600324094\n",
            "epoch 54, loss 0.0006984657375141978\n",
            "epoch 55, loss 0.0006917027058079839\n",
            "epoch 56, loss 0.0006850184290669858\n",
            "epoch 57, loss 0.0006784104625694454\n",
            "epoch 58, loss 0.0006718793883919716\n",
            "epoch 59, loss 0.0006654224125668406\n",
            "epoch 60, loss 0.0006590411649085581\n",
            "epoch 61, loss 0.0006527326186187565\n",
            "epoch 62, loss 0.0006464977632276714\n",
            "epoch 63, loss 0.0006403349689207971\n",
            "epoch 64, loss 0.0006342432461678982\n",
            "epoch 65, loss 0.0006282227113842964\n",
            "epoch 66, loss 0.0006222716765478253\n",
            "epoch 67, loss 0.0006163901998661458\n",
            "epoch 68, loss 0.0006105764186941087\n",
            "epoch 69, loss 0.0006048298673704267\n",
            "epoch 70, loss 0.000599150313064456\n",
            "epoch 71, loss 0.0005935369408689439\n",
            "epoch 72, loss 0.0005879896343685687\n",
            "epoch 73, loss 0.0005825059488415718\n",
            "epoch 74, loss 0.0005770864081569016\n",
            "epoch 75, loss 0.0005717303138226271\n",
            "epoch 76, loss 0.0005664358614012599\n",
            "epoch 77, loss 0.0005612049135379493\n",
            "epoch 78, loss 0.0005560338613577187\n",
            "epoch 79, loss 0.0005509243928827345\n",
            "epoch 80, loss 0.0005458740051835775\n",
            "epoch 81, loss 0.000540883163921535\n",
            "epoch 82, loss 0.0005359508795663714\n",
            "epoch 83, loss 0.0005310766282491386\n",
            "epoch 84, loss 0.0005262591876089573\n",
            "epoch 85, loss 0.0005214984412305057\n",
            "epoch 86, loss 0.000516794272698462\n",
            "epoch 87, loss 0.0005121443537063897\n",
            "epoch 88, loss 0.0005075503722764552\n",
            "epoch 89, loss 0.0005030095344409347\n",
            "epoch 90, loss 0.0004985230043530464\n",
            "epoch 91, loss 0.0004940883372910321\n",
            "epoch 92, loss 0.000489707279484719\n",
            "epoch 93, loss 0.00048537662951275706\n",
            "epoch 94, loss 0.00048109792987816036\n",
            "epoch 95, loss 0.0004768697253894061\n",
            "epoch 96, loss 0.0004726910556200892\n",
            "epoch 97, loss 0.00046856291010044515\n",
            "epoch 98, loss 0.0004644828150048852\n",
            "epoch 99, loss 0.00046045109047554433\n",
            "epoch 100, loss 0.0004564669798128307\n",
            "epoch 101, loss 0.0004525305994320661\n",
            "epoch 102, loss 0.00044864058145321906\n",
            "epoch 103, loss 0.0004447967803571373\n",
            "epoch 104, loss 0.0004409982357174158\n",
            "epoch 105, loss 0.0004372450348455459\n",
            "epoch 106, loss 0.0004335365374572575\n",
            "epoch 107, loss 0.00042987207416445017\n",
            "epoch 108, loss 0.0004262515576556325\n",
            "epoch 109, loss 0.00042267312528565526\n",
            "epoch 110, loss 0.0004191381740383804\n",
            "epoch 111, loss 0.00041564489947631955\n",
            "epoch 112, loss 0.00041219359263777733\n",
            "epoch 113, loss 0.0004087829729542136\n",
            "epoch 114, loss 0.0004054132441524416\n",
            "epoch 115, loss 0.00040208405698649585\n",
            "epoch 116, loss 0.0003987943346146494\n",
            "epoch 117, loss 0.0003955438733100891\n",
            "epoch 118, loss 0.00039233179995790124\n",
            "epoch 119, loss 0.0003891584638040513\n",
            "epoch 120, loss 0.0003860227589029819\n",
            "epoch 121, loss 0.00038292494718916714\n",
            "epoch 122, loss 0.00037986389361321926\n",
            "epoch 123, loss 0.0003768392780330032\n",
            "epoch 124, loss 0.0003738511586561799\n",
            "epoch 125, loss 0.0003708990989252925\n",
            "epoch 126, loss 0.0003679822548292577\n",
            "epoch 127, loss 0.00036510024801827967\n",
            "epoch 128, loss 0.00036225267103873193\n",
            "epoch 129, loss 0.00035943888360634446\n",
            "epoch 130, loss 0.0003566586528904736\n",
            "epoch 131, loss 0.0003539116878528148\n",
            "epoch 132, loss 0.0003511982213240117\n",
            "epoch 133, loss 0.00034851639065891504\n",
            "epoch 134, loss 0.00034586680703796446\n",
            "epoch 135, loss 0.0003432482772041112\n",
            "epoch 136, loss 0.0003406618197914213\n",
            "epoch 137, loss 0.0003381059505045414\n",
            "epoch 138, loss 0.000335581018589437\n",
            "epoch 139, loss 0.00033308585989288986\n",
            "epoch 140, loss 0.00033062059083022177\n",
            "epoch 141, loss 0.0003281847748439759\n",
            "epoch 142, loss 0.0003257776261307299\n",
            "epoch 143, loss 0.0003233999013900757\n",
            "epoch 144, loss 0.00032104982528835535\n",
            "epoch 145, loss 0.00031872803810983896\n",
            "epoch 146, loss 0.00031643439433537424\n",
            "epoch 147, loss 0.00031416784622706473\n",
            "epoch 148, loss 0.00031192830647341907\n",
            "epoch 149, loss 0.00030971577507443726\n",
            "epoch 150, loss 0.00030752975726500154\n",
            "epoch 151, loss 0.00030536975827999413\n",
            "epoch 152, loss 0.00030323609826155007\n",
            "epoch 153, loss 0.0003011274093296379\n",
            "epoch 154, loss 0.00029904398252256215\n",
            "epoch 155, loss 0.0002969852357637137\n",
            "epoch 156, loss 0.00029495154740288854\n",
            "epoch 157, loss 0.0002929417823906988\n",
            "epoch 158, loss 0.0002909563190769404\n",
            "epoch 159, loss 0.00028899440076202154\n",
            "epoch 160, loss 0.0002870562893804163\n",
            "epoch 161, loss 0.0002851414028555155\n",
            "epoch 162, loss 0.0002832494501490146\n",
            "epoch 163, loss 0.00028138014022260904\n",
            "epoch 164, loss 0.0002795330947265029\n",
            "epoch 165, loss 0.00027770816814154387\n",
            "epoch 166, loss 0.00027590495301410556\n",
            "epoch 167, loss 0.0002741235657595098\n",
            "epoch 168, loss 0.00027236356982029974\n",
            "epoch 169, loss 0.000270624557742849\n",
            "epoch 170, loss 0.0002689064422156662\n",
            "epoch 171, loss 0.000267208757577464\n",
            "epoch 172, loss 0.0002655313292052597\n",
            "epoch 173, loss 0.00026387436082586646\n",
            "epoch 174, loss 0.0002622369793243706\n",
            "epoch 175, loss 0.0002606190391816199\n",
            "epoch 176, loss 0.00025902080233208835\n",
            "epoch 177, loss 0.0002574415411800146\n",
            "epoch 178, loss 0.00025588084827177227\n",
            "epoch 179, loss 0.0002543388691265136\n",
            "epoch 180, loss 0.0002528151380829513\n",
            "epoch 181, loss 0.00025130980066023767\n",
            "epoch 182, loss 0.00024982233298942447\n",
            "epoch 183, loss 0.00024835256044752896\n",
            "epoch 184, loss 0.00024690013378858566\n",
            "epoch 185, loss 0.0002454649074934423\n",
            "epoch 186, loss 0.000244046954321675\n",
            "epoch 187, loss 0.00024264557578135282\n",
            "epoch 188, loss 0.00024126145581249148\n",
            "epoch 189, loss 0.0002398933283984661\n",
            "epoch 190, loss 0.00023854161554481834\n",
            "epoch 191, loss 0.0002372060698689893\n",
            "epoch 192, loss 0.0002358862548135221\n",
            "epoch 193, loss 0.00023458227224182338\n",
            "epoch 194, loss 0.00023329371470026672\n",
            "epoch 195, loss 0.00023202074225991964\n",
            "epoch 196, loss 0.00023076272918842733\n",
            "epoch 197, loss 0.0002295192825840786\n",
            "epoch 198, loss 0.00022829073714092374\n",
            "epoch 199, loss 0.00022707706375513226\n",
            "epoch 200, loss 0.0002258775057271123\n",
            "epoch 201, loss 0.00022469226678367704\n",
            "epoch 202, loss 0.00022352123050950468\n",
            "epoch 203, loss 0.00022236400400288403\n",
            "epoch 204, loss 0.00022122033988125622\n",
            "epoch 205, loss 0.000220090412767604\n",
            "epoch 206, loss 0.000218973436858505\n",
            "epoch 207, loss 0.00021787002333439887\n",
            "epoch 208, loss 0.00021677938639186323\n",
            "epoch 209, loss 0.00021570212265942246\n",
            "epoch 210, loss 0.00021463744633365422\n",
            "epoch 211, loss 0.00021358527010306716\n",
            "epoch 212, loss 0.0002125454630004242\n",
            "epoch 213, loss 0.0002115178940584883\n",
            "epoch 214, loss 0.00021050228679087013\n",
            "epoch 215, loss 0.00020949868485331535\n",
            "epoch 216, loss 0.00020850748114753515\n",
            "epoch 217, loss 0.0002075271331705153\n",
            "epoch 218, loss 0.00020655847038142383\n",
            "epoch 219, loss 0.00020560127450153232\n",
            "epoch 220, loss 0.00020465496345423162\n",
            "epoch 221, loss 0.00020372022117953748\n",
            "epoch 222, loss 0.00020279626187402755\n",
            "epoch 223, loss 0.00020188320195302367\n",
            "epoch 224, loss 0.00020098098320886493\n",
            "epoch 225, loss 0.0002000892854994163\n",
            "epoch 226, loss 0.00019920770137105137\n",
            "epoch 227, loss 0.00019833646365441382\n",
            "epoch 228, loss 0.00019747531041502953\n",
            "epoch 229, loss 0.00019662431441247463\n",
            "epoch 230, loss 0.00019578311184886843\n",
            "epoch 231, loss 0.00019495224114507437\n",
            "epoch 232, loss 0.0001941307564266026\n",
            "epoch 233, loss 0.00019331867224536836\n",
            "epoch 234, loss 0.00019251610501669347\n",
            "epoch 235, loss 0.00019172285101376474\n",
            "epoch 236, loss 0.00019093870650976896\n",
            "epoch 237, loss 0.0001901634386740625\n",
            "epoch 238, loss 0.0001893973967526108\n",
            "epoch 239, loss 0.0001886401732917875\n",
            "epoch 240, loss 0.00018789159366860986\n",
            "epoch 241, loss 0.00018715164333116263\n",
            "epoch 242, loss 0.0001864202640717849\n",
            "epoch 243, loss 0.00018569717940408736\n",
            "epoch 244, loss 0.00018498249119147658\n",
            "epoch 245, loss 0.0001842758065322414\n",
            "epoch 246, loss 0.0001835774746723473\n",
            "epoch 247, loss 0.0001828871900215745\n",
            "epoch 248, loss 0.0001822043996071443\n",
            "epoch 249, loss 0.00018152970005758107\n",
            "epoch 250, loss 0.00018086248019244522\n",
            "epoch 251, loss 0.0001802030164981261\n",
            "epoch 252, loss 0.0001795511634554714\n",
            "epoch 253, loss 0.00017890652816276997\n",
            "epoch 254, loss 0.00017826922703534365\n",
            "epoch 255, loss 0.0001776391000021249\n",
            "epoch 256, loss 0.00017701598699204624\n",
            "epoch 257, loss 0.000176399975316599\n",
            "epoch 258, loss 0.00017579102132003754\n",
            "epoch 259, loss 0.00017518899403512478\n",
            "epoch 260, loss 0.00017459384980611503\n",
            "epoch 261, loss 0.00017400537035427988\n",
            "epoch 262, loss 0.0001734233956085518\n",
            "epoch 263, loss 0.00017284795467276126\n",
            "epoch 264, loss 0.00017227878561243415\n",
            "epoch 265, loss 0.00017171591753140092\n",
            "epoch 266, loss 0.00017115937953349203\n",
            "epoch 267, loss 0.0001706090260995552\n",
            "epoch 268, loss 0.0001700645952951163\n",
            "epoch 269, loss 0.00016952645091805607\n",
            "epoch 270, loss 0.00016899425827432424\n",
            "epoch 271, loss 0.0001684680173639208\n",
            "epoch 272, loss 0.00016794745170045644\n",
            "epoch 273, loss 0.00016743277956265956\n",
            "epoch 274, loss 0.0001669237535679713\n",
            "epoch 275, loss 0.0001664202573010698\n",
            "epoch 276, loss 0.00016592210158705711\n",
            "epoch 277, loss 0.00016542954836040735\n",
            "epoch 278, loss 0.0001649421756155789\n",
            "epoch 279, loss 0.00016446018707938492\n",
            "epoch 280, loss 0.00016398346633650362\n",
            "epoch 281, loss 0.00016351210069842637\n",
            "epoch 282, loss 0.0001630458573345095\n",
            "epoch 283, loss 0.0001625845325179398\n",
            "epoch 284, loss 0.00016212809714488685\n",
            "epoch 285, loss 0.0001616767403902486\n",
            "epoch 286, loss 0.00016123027307912707\n",
            "epoch 287, loss 0.00016078853514045477\n",
            "epoch 288, loss 0.0001603516429895535\n",
            "epoch 289, loss 0.00015991942200344056\n",
            "epoch 290, loss 0.00015949177031870931\n",
            "epoch 291, loss 0.00015906858607195318\n",
            "epoch 292, loss 0.00015864992747083306\n",
            "epoch 293, loss 0.00015823576541151851\n",
            "epoch 294, loss 0.00015782585251145065\n",
            "epoch 295, loss 0.0001574203633936122\n",
            "epoch 296, loss 0.00015701944357715547\n",
            "epoch 297, loss 0.00015662272926419973\n",
            "epoch 298, loss 0.00015622992941644043\n",
            "epoch 299, loss 0.0001558413787279278\n",
            "epoch 300, loss 0.0001554569898871705\n",
            "epoch 301, loss 0.00015507653006352484\n",
            "epoch 302, loss 0.00015469983918592334\n",
            "epoch 303, loss 0.00015432709187734872\n",
            "epoch 304, loss 0.000153958288137801\n",
            "epoch 305, loss 0.00015359351527877152\n",
            "epoch 306, loss 0.00015323229308705777\n",
            "epoch 307, loss 0.00015287488349713385\n",
            "epoch 308, loss 0.00015252127195708454\n",
            "epoch 309, loss 0.0001521709345979616\n",
            "epoch 310, loss 0.00015182438073679805\n",
            "epoch 311, loss 0.00015148123202379793\n",
            "epoch 312, loss 0.00015114169218577445\n",
            "epoch 313, loss 0.00015080579032655805\n",
            "epoch 314, loss 0.0001504731917520985\n",
            "epoch 315, loss 0.00015014399832580239\n",
            "epoch 316, loss 0.00014981806452851743\n",
            "epoch 317, loss 0.00014949534670449793\n",
            "epoch 318, loss 0.00014917606313247234\n",
            "epoch 319, loss 0.00014885995187796652\n",
            "epoch 320, loss 0.00014854714390821755\n",
            "epoch 321, loss 0.00014823746460024267\n",
            "epoch 322, loss 0.00014793092850595713\n",
            "epoch 323, loss 0.00014762734645046294\n",
            "epoch 324, loss 0.0001473265147069469\n",
            "epoch 325, loss 0.00014702881162520498\n",
            "epoch 326, loss 0.0001467340625822544\n",
            "epoch 327, loss 0.00014644204929936677\n",
            "epoch 328, loss 0.00014615297550335526\n",
            "epoch 329, loss 0.00014586681209038943\n",
            "epoch 330, loss 0.00014558323891833425\n",
            "epoch 331, loss 0.00014530238695442677\n",
            "epoch 332, loss 0.00014502437261398882\n",
            "epoch 333, loss 0.00014474896306637675\n",
            "epoch 334, loss 0.0001444762892788276\n",
            "epoch 335, loss 0.00014420600200537592\n",
            "epoch 336, loss 0.00014393850869964808\n",
            "epoch 337, loss 0.00014367348921950907\n",
            "epoch 338, loss 0.00014341123460326344\n",
            "epoch 339, loss 0.00014315125008579344\n",
            "epoch 340, loss 0.00014289392856881022\n",
            "epoch 341, loss 0.000142638964462094\n",
            "epoch 342, loss 0.00014238638686947525\n",
            "epoch 343, loss 0.00014213609392754734\n",
            "epoch 344, loss 0.00014188815839588642\n",
            "epoch 345, loss 0.00014164250751491636\n",
            "epoch 346, loss 0.00014139914128463715\n",
            "epoch 347, loss 0.00014115798694547266\n",
            "epoch 348, loss 0.00014091911725699902\n",
            "epoch 349, loss 0.00014068234304431826\n",
            "epoch 350, loss 0.00014044767885934561\n",
            "epoch 351, loss 0.00014021503739058971\n",
            "epoch 352, loss 0.00013998466602060944\n",
            "epoch 353, loss 0.0001397563610225916\n",
            "epoch 354, loss 0.00013953015150036663\n",
            "epoch 355, loss 0.00013930599379818887\n",
            "epoch 356, loss 0.0001390838297083974\n",
            "epoch 357, loss 0.00013886367378290743\n",
            "epoch 358, loss 0.00013864542415831238\n",
            "epoch 359, loss 0.0001384290080750361\n",
            "epoch 360, loss 0.00013821464381180704\n",
            "epoch 361, loss 0.00013800233136862516\n",
            "epoch 362, loss 0.00013779191067442298\n",
            "epoch 363, loss 0.00013758332352153957\n",
            "epoch 364, loss 0.0001373764534946531\n",
            "epoch 365, loss 0.0001371714024571702\n",
            "epoch 366, loss 0.00013696808309759945\n",
            "epoch 367, loss 0.00013676664093509316\n",
            "epoch 368, loss 0.00013656681403517723\n",
            "epoch 369, loss 0.0001363689370919019\n",
            "epoch 370, loss 0.0001361727190669626\n",
            "epoch 371, loss 0.00013597811630461365\n",
            "epoch 372, loss 0.0001357850560452789\n",
            "epoch 373, loss 0.00013559359649661928\n",
            "epoch 374, loss 0.00013540378131438047\n",
            "epoch 375, loss 0.00013521539221983403\n",
            "epoch 376, loss 0.00013502861838787794\n",
            "epoch 377, loss 0.00013484327064361423\n",
            "epoch 378, loss 0.00013465978554449975\n",
            "epoch 379, loss 0.0001344775955658406\n",
            "epoch 380, loss 0.00013429693353828043\n",
            "epoch 381, loss 0.00013411781401373446\n",
            "epoch 382, loss 0.00013394006236921996\n",
            "epoch 383, loss 0.00013376367860473692\n",
            "epoch 384, loss 0.00013358872092794627\n",
            "epoch 385, loss 0.00013341523299459368\n",
            "epoch 386, loss 0.00013324312749318779\n",
            "epoch 387, loss 0.00013307234621606767\n",
            "epoch 388, loss 0.0001329029764747247\n",
            "epoch 389, loss 0.00013273503282107413\n",
            "epoch 390, loss 0.0001325682969763875\n",
            "epoch 391, loss 0.00013240300177130848\n",
            "epoch 392, loss 0.00013223904534243047\n",
            "epoch 393, loss 0.00013207645679358393\n",
            "epoch 394, loss 0.00013191494508646429\n",
            "epoch 395, loss 0.00013175475760363042\n",
            "epoch 396, loss 0.00013159567606635392\n",
            "epoch 397, loss 0.00013143780233804137\n",
            "epoch 398, loss 0.0001312813110416755\n",
            "epoch 399, loss 0.0001311258674832061\n",
            "epoch 400, loss 0.0001309715589741245\n",
            "epoch 401, loss 0.00013081867655273527\n",
            "epoch 402, loss 0.0001306669437326491\n",
            "epoch 403, loss 0.0001305161858908832\n",
            "epoch 404, loss 0.00013036656309850514\n",
            "epoch 405, loss 0.00013021811901126057\n",
            "epoch 406, loss 0.0001300707517657429\n",
            "epoch 407, loss 0.00012992428673896939\n",
            "epoch 408, loss 0.0001297791168326512\n",
            "epoch 409, loss 0.000129634965560399\n",
            "epoch 410, loss 0.00012949196388944983\n",
            "epoch 411, loss 0.0001293498498853296\n",
            "epoch 412, loss 0.00012920878361910582\n",
            "epoch 413, loss 0.0001290687796426937\n",
            "epoch 414, loss 0.00012892972154077142\n",
            "epoch 415, loss 0.0001287915656575933\n",
            "epoch 416, loss 0.00012865447206422687\n",
            "epoch 417, loss 0.00012851845531258732\n",
            "epoch 418, loss 0.00012838328257203102\n",
            "epoch 419, loss 0.0001282488665310666\n",
            "epoch 420, loss 0.0001281153818126768\n",
            "epoch 421, loss 0.00012798266834579408\n",
            "epoch 422, loss 0.00012785091530531645\n",
            "epoch 423, loss 0.00012771996262017637\n",
            "epoch 424, loss 0.0001275900867767632\n",
            "epoch 425, loss 0.0001274611713597551\n",
            "epoch 426, loss 0.00012733304174616933\n",
            "epoch 427, loss 0.00012720584345515817\n",
            "epoch 428, loss 0.00012707934365607798\n",
            "epoch 429, loss 0.00012695368786808103\n",
            "epoch 430, loss 0.00012682884698733687\n",
            "epoch 431, loss 0.0001267048646695912\n",
            "epoch 432, loss 0.00012658166815526783\n",
            "epoch 433, loss 0.00012645938841160387\n",
            "epoch 434, loss 0.00012633799633476883\n",
            "epoch 435, loss 0.00012621720088645816\n",
            "epoch 436, loss 0.00012609710393007845\n",
            "epoch 437, loss 0.0001259779674001038\n",
            "epoch 438, loss 0.00012585950025822967\n",
            "epoch 439, loss 0.0001257417316082865\n",
            "epoch 440, loss 0.000125625025248155\n",
            "epoch 441, loss 0.00012550897372420877\n",
            "epoch 442, loss 0.00012539350427687168\n",
            "epoch 443, loss 0.00012527895160019398\n",
            "epoch 444, loss 0.00012516503920778632\n",
            "epoch 445, loss 0.0001250515488209203\n",
            "epoch 446, loss 0.00012493893154896796\n",
            "epoch 447, loss 0.00012482677993830293\n",
            "epoch 448, loss 0.00012471557420212775\n",
            "epoch 449, loss 0.0001246049941983074\n",
            "epoch 450, loss 0.00012449498171918094\n",
            "epoch 451, loss 0.000124385638628155\n",
            "epoch 452, loss 0.00012427690671756864\n",
            "epoch 453, loss 0.00012416882964316756\n",
            "epoch 454, loss 0.00012406142195686698\n",
            "epoch 455, loss 0.000123954625451006\n",
            "epoch 456, loss 0.00012384849833324552\n",
            "epoch 457, loss 0.0001237428659806028\n",
            "epoch 458, loss 0.00012363787391223013\n",
            "epoch 459, loss 0.00012353334750514477\n",
            "epoch 460, loss 0.0001234295341419056\n",
            "epoch 461, loss 0.00012332598271314055\n",
            "epoch 462, loss 0.00012322323163971305\n",
            "epoch 463, loss 0.00012312107719480991\n",
            "epoch 464, loss 0.00012301972310524434\n",
            "epoch 465, loss 0.00012291892198845744\n",
            "epoch 466, loss 0.0001228185137733817\n",
            "epoch 467, loss 0.00012271874584257603\n",
            "epoch 468, loss 0.0001226194144692272\n",
            "epoch 469, loss 0.00012252049054950476\n",
            "epoch 470, loss 0.00012242220691405237\n",
            "epoch 471, loss 0.0001223245490109548\n",
            "epoch 472, loss 0.0001222274440806359\n",
            "epoch 473, loss 0.00012213077570777386\n",
            "epoch 474, loss 0.00012203458027215675\n",
            "epoch 475, loss 0.00012193879229016602\n",
            "epoch 476, loss 0.00012184351362520829\n",
            "epoch 477, loss 0.00012174876610515639\n",
            "epoch 478, loss 0.0001216545861097984\n",
            "epoch 479, loss 0.00012156090087955818\n",
            "epoch 480, loss 0.00012146781227784231\n",
            "epoch 481, loss 0.00012137501471443102\n",
            "epoch 482, loss 0.00012128285015933216\n",
            "epoch 483, loss 0.00012119101302232593\n",
            "epoch 484, loss 0.0001210995324072428\n",
            "epoch 485, loss 0.00012100858293706551\n",
            "epoch 486, loss 0.00012091814278392121\n",
            "epoch 487, loss 0.00012082803004886955\n",
            "epoch 488, loss 0.00012073837569914758\n",
            "epoch 489, loss 0.00012064913607900962\n",
            "epoch 490, loss 0.00012056034029228613\n",
            "epoch 491, loss 0.00012047174823237583\n",
            "epoch 492, loss 0.00012038376007694751\n",
            "epoch 493, loss 0.00012029617937514558\n",
            "epoch 494, loss 0.00012020888243569061\n",
            "epoch 495, loss 0.00012012203660560772\n",
            "epoch 496, loss 0.00012003580923192203\n",
            "epoch 497, loss 0.00011994985834462568\n",
            "epoch 498, loss 0.00011986424215137959\n",
            "epoch 499, loss 0.00011977893154835328\n",
            "epoch 500, loss 0.00011969406477874145\n",
            "epoch 501, loss 0.00011960967094637454\n",
            "epoch 502, loss 0.0001195253717014566\n",
            "epoch 503, loss 0.00011944179277634248\n",
            "epoch 504, loss 0.00011935847578570247\n",
            "epoch 505, loss 0.00011927552986890078\n",
            "epoch 506, loss 0.00011919287499040365\n",
            "epoch 507, loss 0.0001191105111502111\n",
            "epoch 508, loss 0.00011902857659151778\n",
            "epoch 509, loss 0.00011894691124325618\n",
            "epoch 510, loss 0.00011886562424479052\n",
            "epoch 511, loss 0.0001187848174595274\n",
            "epoch 512, loss 0.00011870434536831453\n",
            "epoch 513, loss 0.00011862417886732146\n",
            "epoch 514, loss 0.00011854431068059057\n",
            "epoch 515, loss 0.00011846479173982516\n",
            "epoch 516, loss 0.00011838570208055899\n",
            "epoch 517, loss 0.00011830674338852987\n",
            "epoch 518, loss 0.00011822805390693247\n",
            "epoch 519, loss 0.00011814996105385944\n",
            "epoch 520, loss 0.00011807221017079428\n",
            "epoch 521, loss 0.000117994706670288\n",
            "epoch 522, loss 0.00011791758879553527\n",
            "epoch 523, loss 0.00011784092930611223\n",
            "epoch 524, loss 0.00011776448081946\n",
            "epoch 525, loss 0.00011768831609515473\n",
            "epoch 526, loss 0.00011761230416595936\n",
            "epoch 527, loss 0.00011753675789805129\n",
            "epoch 528, loss 0.00011746142263291404\n",
            "epoch 529, loss 0.00011738631292246282\n",
            "epoch 530, loss 0.00011731156700989231\n",
            "epoch 531, loss 0.00011723717034328729\n",
            "epoch 532, loss 0.00011716292647179216\n",
            "epoch 533, loss 0.00011708918464137241\n",
            "epoch 534, loss 0.00011701537005137652\n",
            "epoch 535, loss 0.00011694204295054078\n",
            "epoch 536, loss 0.00011686891230056062\n",
            "epoch 537, loss 0.00011679599265335128\n",
            "epoch 538, loss 0.00011672349501168355\n",
            "epoch 539, loss 0.00011665122292470187\n",
            "epoch 540, loss 0.00011657917639240623\n",
            "epoch 541, loss 0.00011650731903500855\n",
            "epoch 542, loss 0.00011643579637166113\n",
            "epoch 543, loss 0.00011636439739959314\n",
            "epoch 544, loss 0.00011629333312157542\n",
            "epoch 545, loss 0.00011622254532994702\n",
            "epoch 546, loss 0.00011615197581704706\n",
            "epoch 547, loss 0.00011608181375777349\n",
            "epoch 548, loss 0.00011601176811382174\n",
            "epoch 549, loss 0.0001159418752649799\n",
            "epoch 550, loss 0.00011587235348997638\n",
            "epoch 551, loss 0.00011580296995816752\n",
            "epoch 552, loss 0.00011573384836083278\n",
            "epoch 553, loss 0.00011566495959414169\n",
            "epoch 554, loss 0.00011559626000234857\n",
            "epoch 555, loss 0.00011552782234502956\n",
            "epoch 556, loss 0.00011545944289537147\n",
            "epoch 557, loss 0.00011539150727912784\n",
            "epoch 558, loss 0.0001153235716628842\n",
            "epoch 559, loss 0.00011525594891281798\n",
            "epoch 560, loss 0.00011518863175297156\n",
            "epoch 561, loss 0.00011512153287185356\n",
            "epoch 562, loss 0.00011505470320116729\n",
            "epoch 563, loss 0.0001149879171862267\n",
            "epoch 564, loss 0.0001149214367615059\n",
            "epoch 565, loss 0.00011485504364827648\n",
            "epoch 566, loss 0.00011478896340122446\n",
            "epoch 567, loss 0.00011472307232907042\n",
            "epoch 568, loss 0.00011465763964224607\n",
            "epoch 569, loss 0.00011459227243904024\n",
            "epoch 570, loss 0.00011452709441073239\n",
            "epoch 571, loss 0.00011446204734966159\n",
            "epoch 572, loss 0.00011439724767114967\n",
            "epoch 573, loss 0.00011433265171945095\n",
            "epoch 574, loss 0.00011426817945903167\n",
            "epoch 575, loss 0.00011420396185712889\n",
            "epoch 576, loss 0.00011413998436182737\n",
            "epoch 577, loss 0.00011407626152504236\n",
            "epoch 578, loss 0.00011401278607081622\n",
            "epoch 579, loss 0.00011394923785701394\n",
            "epoch 580, loss 0.000113885966129601\n",
            "epoch 581, loss 0.00011382273078197613\n",
            "epoch 582, loss 0.00011375973554095253\n",
            "epoch 583, loss 0.0001136968785431236\n",
            "epoch 584, loss 0.00011363442899892107\n",
            "epoch 585, loss 0.00011357216135365888\n",
            "epoch 586, loss 0.00011351010471116751\n",
            "epoch 587, loss 0.00011344804079271853\n",
            "epoch 588, loss 0.0001133864134317264\n",
            "epoch 589, loss 0.00011332471331115812\n",
            "epoch 590, loss 0.00011326311505399644\n",
            "epoch 591, loss 0.00011320180783513933\n",
            "epoch 592, loss 0.00011314075527479872\n",
            "epoch 593, loss 0.00011307979730190709\n",
            "epoch 594, loss 0.00011301915947115049\n",
            "epoch 595, loss 0.00011295851436443627\n",
            "epoch 596, loss 0.00011289798567304388\n",
            "epoch 597, loss 0.0001128377189161256\n",
            "epoch 598, loss 0.00011277755402261391\n",
            "epoch 599, loss 0.00011271763651166111\n",
            "epoch 600, loss 0.0001126579663832672\n",
            "epoch 601, loss 0.00011259822349529713\n",
            "epoch 602, loss 0.0001125388516811654\n",
            "epoch 603, loss 0.00011247980000916868\n",
            "epoch 604, loss 0.00011242067557759583\n",
            "epoch 605, loss 0.00011236176214879379\n",
            "epoch 606, loss 0.00011230295058339834\n",
            "epoch 607, loss 0.00011224430636502802\n",
            "epoch 608, loss 0.00011218589497730136\n",
            "epoch 609, loss 0.00011212760728085414\n",
            "epoch 610, loss 0.00011206971248611808\n",
            "epoch 611, loss 0.00011201188317500055\n",
            "epoch 612, loss 0.00011195414117537439\n",
            "epoch 613, loss 0.00011189642827957869\n",
            "epoch 614, loss 0.00011183907918166369\n",
            "epoch 615, loss 0.00011178185377502814\n",
            "epoch 616, loss 0.00011172459926456213\n",
            "epoch 617, loss 0.00011166757030878216\n",
            "epoch 618, loss 0.00011161087604705244\n",
            "epoch 619, loss 0.00011155427637277171\n",
            "epoch 620, loss 0.00011149777856189758\n",
            "epoch 621, loss 0.00011144123709527776\n",
            "epoch 622, loss 0.00011138489207951352\n",
            "epoch 623, loss 0.00011132891813758761\n",
            "epoch 624, loss 0.00011127290781587362\n",
            "epoch 625, loss 0.0001112169265979901\n",
            "epoch 626, loss 0.00011116125824628398\n",
            "epoch 627, loss 0.00011110567720606923\n",
            "epoch 628, loss 0.00011105011799372733\n",
            "epoch 629, loss 0.00011099490075139329\n",
            "epoch 630, loss 0.00011093961802544072\n",
            "epoch 631, loss 0.00011088449537055567\n",
            "epoch 632, loss 0.00011082954006269574\n",
            "epoch 633, loss 0.0001107747302739881\n",
            "epoch 634, loss 0.00011072005145251751\n",
            "epoch 635, loss 0.0001106655690819025\n",
            "epoch 636, loss 0.00011061124678235501\n",
            "epoch 637, loss 0.00011055693903472275\n",
            "epoch 638, loss 0.00011050280591007322\n",
            "epoch 639, loss 0.00011044876009691507\n",
            "epoch 640, loss 0.00011039492528652772\n",
            "epoch 641, loss 0.0001103409958886914\n",
            "epoch 642, loss 0.00011028747394448146\n",
            "epoch 643, loss 0.00011023405386367813\n",
            "epoch 644, loss 0.00011018060467904434\n",
            "epoch 645, loss 0.00011012729373760521\n",
            "epoch 646, loss 0.00011007411376340315\n",
            "epoch 647, loss 0.00011002102837665007\n",
            "epoch 648, loss 0.0001099680521292612\n",
            "epoch 649, loss 0.00010991523595293984\n",
            "epoch 650, loss 0.00010986241977661848\n",
            "epoch 651, loss 0.00010980980732711032\n",
            "epoch 652, loss 0.00010975730401696637\n",
            "epoch 653, loss 0.00010970501170959324\n",
            "epoch 654, loss 0.00010965273395413533\n",
            "epoch 655, loss 0.0001096006017178297\n",
            "epoch 656, loss 0.00010954859317280352\n",
            "epoch 657, loss 0.00010949683928629383\n",
            "epoch 658, loss 0.00010944509995169938\n",
            "epoch 659, loss 0.00010939346248051152\n",
            "epoch 660, loss 0.00010934186866506934\n",
            "epoch 661, loss 0.00010929052223218605\n",
            "epoch 662, loss 0.00010923927038675174\n",
            "epoch 663, loss 0.00010918806947302073\n",
            "epoch 664, loss 0.00010913698497461155\n",
            "epoch 665, loss 0.00010908599506365135\n",
            "epoch 666, loss 0.00010903507063630968\n",
            "epoch 667, loss 0.00010898426990024745\n",
            "epoch 668, loss 0.00010893368744291365\n",
            "epoch 669, loss 0.00010888306860579178\n",
            "epoch 670, loss 0.00010883267532335594\n",
            "epoch 671, loss 0.00010878226748900488\n",
            "epoch 672, loss 0.00010873204155359417\n",
            "epoch 673, loss 0.00010868194658542052\n",
            "epoch 674, loss 0.00010863196803256869\n",
            "epoch 675, loss 0.00010858209134312347\n",
            "epoch 676, loss 0.0001085322437575087\n",
            "epoch 677, loss 0.00010848246165551245\n",
            "epoch 678, loss 0.00010843287600437179\n",
            "epoch 679, loss 0.00010838353773579001\n",
            "epoch 680, loss 0.0001083341776393354\n",
            "epoch 681, loss 0.00010828507220139727\n",
            "epoch 682, loss 0.00010823588672792539\n",
            "epoch 683, loss 0.00010818711598403752\n",
            "epoch 684, loss 0.00010813812696142122\n",
            "epoch 685, loss 0.00010808924707816914\n",
            "epoch 686, loss 0.00010804051271406934\n",
            "epoch 687, loss 0.00010799199662869796\n",
            "epoch 688, loss 0.00010794353147502989\n",
            "epoch 689, loss 0.00010789519001264125\n",
            "epoch 690, loss 0.00010784678306663409\n",
            "epoch 691, loss 0.00010779870353871956\n",
            "epoch 692, loss 0.0001077506531146355\n",
            "epoch 693, loss 0.00010770252993097529\n",
            "epoch 694, loss 0.00010765469050966203\n",
            "epoch 695, loss 0.00010760679288068786\n",
            "epoch 696, loss 0.00010755908442661166\n",
            "epoch 697, loss 0.00010751131048891693\n",
            "epoch 698, loss 0.0001074637912097387\n",
            "epoch 699, loss 0.00010741641017375514\n",
            "epoch 700, loss 0.00010736893455032259\n",
            "epoch 701, loss 0.00010732158261816949\n",
            "epoch 702, loss 0.00010727430344559252\n",
            "epoch 703, loss 0.00010722708975663409\n",
            "epoch 704, loss 0.00010718015255406499\n",
            "epoch 705, loss 0.00010713331721490249\n",
            "epoch 706, loss 0.00010708653280744329\n",
            "epoch 707, loss 0.00010703987936722115\n",
            "epoch 708, loss 0.00010699325503082946\n",
            "epoch 709, loss 0.0001069468489731662\n",
            "epoch 710, loss 0.00010690039198379964\n",
            "epoch 711, loss 0.00010685398592613637\n",
            "epoch 712, loss 0.0001068075725925155\n",
            "epoch 713, loss 0.0001067614066414535\n",
            "epoch 714, loss 0.0001067152334144339\n",
            "epoch 715, loss 0.00010666926391422749\n",
            "epoch 716, loss 0.00010662349086487666\n",
            "epoch 717, loss 0.00010657768143573776\n",
            "epoch 718, loss 0.00010653181379893795\n",
            "epoch 719, loss 0.00010648625902831554\n",
            "epoch 720, loss 0.00010644068970577791\n",
            "epoch 721, loss 0.00010639499669196084\n",
            "epoch 722, loss 0.00010634953650878742\n",
            "epoch 723, loss 0.00010630438919179142\n",
            "epoch 724, loss 0.00010625919094309211\n",
            "epoch 725, loss 0.00010621397814247757\n",
            "epoch 726, loss 0.00010616908548399806\n",
            "epoch 727, loss 0.00010612399637466297\n",
            "epoch 728, loss 0.00010607916192384437\n",
            "epoch 729, loss 0.00010603428381728008\n",
            "epoch 730, loss 0.00010598954395391047\n",
            "epoch 731, loss 0.00010594492050586268\n",
            "epoch 732, loss 0.00010590026795398444\n",
            "epoch 733, loss 0.00010585594282019883\n",
            "epoch 734, loss 0.00010581140668364242\n",
            "epoch 735, loss 0.000105767379864119\n",
            "epoch 736, loss 0.00010572308383416384\n",
            "epoch 737, loss 0.00010567892604740337\n",
            "epoch 738, loss 0.00010563479008851573\n",
            "epoch 739, loss 0.00010559073416516185\n",
            "epoch 740, loss 0.00010554691834840924\n",
            "epoch 741, loss 0.00010550307342782617\n",
            "epoch 742, loss 0.00010545942495809868\n",
            "epoch 743, loss 0.00010541597293922678\n",
            "epoch 744, loss 0.00010537241178099066\n",
            "epoch 745, loss 0.00010532905434956774\n",
            "epoch 746, loss 0.00010528572602197528\n",
            "epoch 747, loss 0.00010524246317800134\n",
            "epoch 748, loss 0.00010519928036956117\n",
            "epoch 749, loss 0.00010515599569771439\n",
            "epoch 750, loss 0.00010511292930459604\n",
            "epoch 751, loss 0.00010506998660275713\n",
            "epoch 752, loss 0.00010502716759219766\n",
            "epoch 753, loss 0.0001049844577210024\n",
            "epoch 754, loss 0.00010494174784980714\n",
            "epoch 755, loss 0.00010489912529010326\n",
            "epoch 756, loss 0.00010485644452273846\n",
            "epoch 757, loss 0.00010481389472261071\n",
            "epoch 758, loss 0.00010477136675035581\n",
            "epoch 759, loss 0.00010472873691469431\n",
            "epoch 760, loss 0.00010468647087691352\n",
            "epoch 761, loss 0.00010464438673807308\n",
            "epoch 762, loss 0.0001046023317030631\n",
            "epoch 763, loss 0.00010456028394401073\n",
            "epoch 764, loss 0.00010451830894453451\n",
            "epoch 765, loss 0.0001044762393576093\n",
            "epoch 766, loss 0.0001044343298417516\n",
            "epoch 767, loss 0.00010439247853355482\n",
            "epoch 768, loss 0.00010435069270897657\n",
            "epoch 769, loss 0.00010430905967950821\n",
            "epoch 770, loss 0.00010426753578940406\n",
            "epoch 771, loss 0.00010422613559057936\n",
            "epoch 772, loss 0.00010418474994366989\n",
            "epoch 773, loss 0.00010414329153718427\n",
            "epoch 774, loss 0.00010410214599687606\n",
            "epoch 775, loss 0.00010406078945379704\n",
            "epoch 776, loss 0.00010401957842987031\n",
            "epoch 777, loss 0.00010397839650977403\n",
            "epoch 778, loss 0.00010393733100499958\n",
            "epoch 779, loss 0.00010389634553575888\n",
            "epoch 780, loss 0.00010385562927694991\n",
            "epoch 781, loss 0.0001038148402585648\n",
            "epoch 782, loss 0.00010377419675933197\n",
            "epoch 783, loss 0.00010373374243499711\n",
            "epoch 784, loss 0.00010369317897129804\n",
            "epoch 785, loss 0.00010365267371525988\n",
            "epoch 786, loss 0.00010361237218603492\n",
            "epoch 787, loss 0.00010357189603382722\n",
            "epoch 788, loss 0.00010353170364396647\n",
            "epoch 789, loss 0.00010349164222134277\n",
            "epoch 790, loss 0.00010345158079871908\n",
            "epoch 791, loss 0.00010341170127503574\n",
            "epoch 792, loss 0.00010337177809560671\n",
            "epoch 793, loss 0.00010333192039979622\n",
            "epoch 794, loss 0.00010329213546356186\n",
            "epoch 795, loss 0.00010325220500817522\n",
            "epoch 796, loss 0.00010321257286705077\n",
            "epoch 797, loss 0.00010317291162209585\n",
            "epoch 798, loss 0.00010313341044820845\n",
            "epoch 799, loss 0.00010309393110219389\n",
            "epoch 800, loss 0.00010305469913873821\n",
            "epoch 801, loss 0.00010301540169166401\n",
            "epoch 802, loss 0.00010297622065991163\n",
            "epoch 803, loss 0.00010293690138496459\n",
            "epoch 804, loss 0.0001028979240800254\n",
            "epoch 805, loss 0.00010285869939252734\n",
            "epoch 806, loss 0.00010281953291269019\n",
            "epoch 807, loss 0.00010278051195200533\n",
            "epoch 808, loss 0.00010274155647493899\n",
            "epoch 809, loss 0.00010270281927660108\n",
            "epoch 810, loss 0.00010266390745528042\n",
            "epoch 811, loss 0.0001026251629809849\n",
            "epoch 812, loss 0.00010258660768158734\n",
            "epoch 813, loss 0.0001025478559313342\n",
            "epoch 814, loss 0.00010250927152810618\n",
            "epoch 815, loss 0.00010247074533253908\n",
            "epoch 816, loss 0.00010243234282825142\n",
            "epoch 817, loss 0.00010239410039503127\n",
            "epoch 818, loss 0.000102355690614786\n",
            "epoch 819, loss 0.00010231753549305722\n",
            "epoch 820, loss 0.00010227938037132844\n",
            "epoch 821, loss 0.00010224131256109104\n",
            "epoch 822, loss 0.00010220326657872647\n",
            "epoch 823, loss 0.00010216523514827713\n",
            "epoch 824, loss 0.00010212728375336155\n",
            "epoch 825, loss 0.0001020893469103612\n",
            "epoch 826, loss 0.00010205134458374232\n",
            "epoch 827, loss 0.00010201363329542801\n",
            "epoch 828, loss 0.00010197590017924085\n",
            "epoch 829, loss 0.00010193824709858745\n",
            "epoch 830, loss 0.00010190070315729827\n",
            "epoch 831, loss 0.00010186322469962761\n",
            "epoch 832, loss 0.00010182585538132116\n",
            "epoch 833, loss 0.00010178836964769289\n",
            "epoch 834, loss 0.00010175106581300497\n",
            "epoch 835, loss 0.00010171372559852898\n",
            "epoch 836, loss 0.00010167655273107812\n",
            "epoch 837, loss 0.00010163946717511863\n",
            "epoch 838, loss 0.0001016023670672439\n",
            "epoch 839, loss 0.00010156554344575852\n",
            "epoch 840, loss 0.00010152877075597644\n",
            "epoch 841, loss 0.00010149201261810958\n",
            "epoch 842, loss 0.00010145520354853943\n",
            "epoch 843, loss 0.00010141841630684212\n",
            "epoch 844, loss 0.00010138177458429709\n",
            "epoch 845, loss 0.00010134504555026069\n",
            "epoch 846, loss 0.00010130855662282556\n",
            "epoch 847, loss 0.00010127203131560236\n",
            "epoch 848, loss 0.000101235527836252\n",
            "epoch 849, loss 0.00010119911894435063\n",
            "epoch 850, loss 0.00010116269550053403\n",
            "epoch 851, loss 0.00010112642485182732\n",
            "epoch 852, loss 0.00010109013965120539\n",
            "epoch 853, loss 0.00010105386172654107\n",
            "epoch 854, loss 0.00010101764200953767\n",
            "epoch 855, loss 0.00010098141501657665\n",
            "epoch 856, loss 0.00010094542085425928\n",
            "epoch 857, loss 0.0001009093175525777\n",
            "epoch 858, loss 0.0001008732506306842\n",
            "epoch 859, loss 0.00010083730740007013\n",
            "epoch 860, loss 0.00010080144420498982\n",
            "epoch 861, loss 0.00010076585022034124\n",
            "epoch 862, loss 0.00010072996519738808\n",
            "epoch 863, loss 0.00010069427662529051\n",
            "epoch 864, loss 0.00010065879905596375\n",
            "epoch 865, loss 0.0001006232196232304\n",
            "epoch 866, loss 0.00010058764019049704\n",
            "epoch 867, loss 0.00010055216262117028\n",
            "epoch 868, loss 0.00010051691060652956\n",
            "epoch 869, loss 0.00010048146214103326\n",
            "epoch 870, loss 0.00010044635564554483\n",
            "epoch 871, loss 0.00010041122004622594\n",
            "epoch 872, loss 0.00010037608444690704\n",
            "epoch 873, loss 0.00010034088336396962\n",
            "epoch 874, loss 0.0001003058860078454\n",
            "epoch 875, loss 0.00010027088137576357\n",
            "epoch 876, loss 0.0001002358112600632\n",
            "epoch 877, loss 0.00010020076297223568\n",
            "epoch 878, loss 0.00010016576561611146\n",
            "epoch 879, loss 0.00010013112478191033\n",
            "epoch 880, loss 0.00010009631660068408\n",
            "epoch 881, loss 0.00010006177762988955\n",
            "epoch 882, loss 0.00010002713679568842\n",
            "epoch 883, loss 9.999261237680912e-05\n",
            "epoch 884, loss 9.99581825453788e-05\n",
            "epoch 885, loss 9.992371633416042e-05\n",
            "epoch 886, loss 9.98893374344334e-05\n",
            "epoch 887, loss 9.985493670683354e-05\n",
            "epoch 888, loss 9.982071787817404e-05\n",
            "epoch 889, loss 9.97863826341927e-05\n",
            "epoch 890, loss 9.975224384106696e-05\n",
            "epoch 891, loss 9.971807594411075e-05\n",
            "epoch 892, loss 9.968411177396774e-05\n",
            "epoch 893, loss 9.964984201360494e-05\n",
            "epoch 894, loss 9.961598698282614e-05\n",
            "epoch 895, loss 9.958193550119177e-05\n",
            "epoch 896, loss 9.954796405509114e-05\n",
            "epoch 897, loss 9.951410174835473e-05\n",
            "epoch 898, loss 9.94802758214064e-05\n",
            "epoch 899, loss 9.944648627424613e-05\n",
            "epoch 900, loss 9.941257303580642e-05\n",
            "epoch 901, loss 9.937878348864615e-05\n",
            "epoch 902, loss 9.93453140836209e-05\n",
            "epoch 903, loss 9.931177191901952e-05\n",
            "epoch 904, loss 9.927820065058768e-05\n",
            "epoch 905, loss 9.924483310896903e-05\n",
            "epoch 906, loss 9.921135642798617e-05\n",
            "epoch 907, loss 9.917800343828276e-05\n",
            "epoch 908, loss 9.914469410432503e-05\n",
            "epoch 909, loss 9.91112829069607e-05\n",
            "epoch 910, loss 9.907794446917251e-05\n",
            "epoch 911, loss 9.904469334287569e-05\n",
            "epoch 912, loss 9.901144221657887e-05\n",
            "epoch 913, loss 9.897851123241708e-05\n",
            "epoch 914, loss 9.89454856608063e-05\n",
            "epoch 915, loss 9.891246008919552e-05\n",
            "epoch 916, loss 9.887935448205099e-05\n",
            "epoch 917, loss 9.884656174108386e-05\n",
            "epoch 918, loss 9.881355072138831e-05\n",
            "epoch 919, loss 9.878082346403971e-05\n",
            "epoch 920, loss 9.874808165477589e-05\n",
            "epoch 921, loss 9.871517249848694e-05\n",
            "epoch 922, loss 9.868254710454494e-05\n",
            "epoch 923, loss 9.865003812592477e-05\n",
            "epoch 924, loss 9.861754369921982e-05\n",
            "epoch 925, loss 9.858503472059965e-05\n",
            "epoch 926, loss 9.855265670921654e-05\n",
            "epoch 927, loss 9.852030780166388e-05\n",
            "epoch 928, loss 9.848792979028076e-05\n",
            "epoch 929, loss 9.845558815868571e-05\n",
            "epoch 930, loss 9.842330473475158e-05\n",
            "epoch 931, loss 9.839107224252075e-05\n",
            "epoch 932, loss 9.835887613007799e-05\n",
            "epoch 933, loss 9.832664363784716e-05\n",
            "epoch 934, loss 9.82945566647686e-05\n",
            "epoch 935, loss 9.826252789935097e-05\n",
            "epoch 936, loss 9.823059372138232e-05\n",
            "epoch 937, loss 9.819857950787991e-05\n",
            "epoch 938, loss 9.816655801841989e-05\n",
            "epoch 939, loss 9.813472570385784e-05\n",
            "epoch 940, loss 9.810280607780442e-05\n",
            "epoch 941, loss 9.807085734792054e-05\n",
            "epoch 942, loss 9.803906141314656e-05\n",
            "epoch 943, loss 9.800727275433019e-05\n",
            "epoch 944, loss 9.797552047530189e-05\n",
            "epoch 945, loss 9.794373909244314e-05\n",
            "epoch 946, loss 9.79122196440585e-05\n",
            "epoch 947, loss 9.78808748186566e-05\n",
            "epoch 948, loss 9.784929716261104e-05\n",
            "epoch 949, loss 9.78178286459297e-05\n",
            "epoch 950, loss 9.778644744073972e-05\n",
            "epoch 951, loss 9.775511716725305e-05\n",
            "epoch 952, loss 9.77238014456816e-05\n",
            "epoch 953, loss 9.769256575964391e-05\n",
            "epoch 954, loss 9.766135190147907e-05\n",
            "epoch 955, loss 9.763021080289036e-05\n",
            "epoch 956, loss 9.75989896687679e-05\n",
            "epoch 957, loss 9.756800136528909e-05\n",
            "epoch 958, loss 9.753699850989506e-05\n",
            "epoch 959, loss 9.75059883785434e-05\n",
            "epoch 960, loss 9.747513831825927e-05\n",
            "epoch 961, loss 9.744416456669569e-05\n",
            "epoch 962, loss 9.741341636981815e-05\n",
            "epoch 963, loss 9.73824062384665e-05\n",
            "epoch 964, loss 9.735179628478363e-05\n",
            "epoch 965, loss 9.732104808790609e-05\n",
            "epoch 966, loss 9.729039447847754e-05\n",
            "epoch 967, loss 9.725963172968477e-05\n",
            "epoch 968, loss 9.722918184706941e-05\n",
            "epoch 969, loss 9.719873924041167e-05\n",
            "epoch 970, loss 9.716846398077905e-05\n",
            "epoch 971, loss 9.713819599710405e-05\n",
            "epoch 972, loss 9.710803715279326e-05\n",
            "epoch 973, loss 9.70777400652878e-05\n",
            "epoch 974, loss 9.704759577289224e-05\n",
            "epoch 975, loss 9.701745148049667e-05\n",
            "epoch 976, loss 9.698744543129578e-05\n",
            "epoch 977, loss 9.695732296677306e-05\n",
            "epoch 978, loss 9.692730236565694e-05\n",
            "epoch 979, loss 9.689746366348118e-05\n",
            "epoch 980, loss 9.686738485470414e-05\n",
            "epoch 981, loss 9.683737880550325e-05\n",
            "epoch 982, loss 9.680756920715794e-05\n",
            "epoch 983, loss 9.677773778093979e-05\n",
            "epoch 984, loss 9.674797183834016e-05\n",
            "epoch 985, loss 9.67182859312743e-05\n",
            "epoch 986, loss 9.668840357335284e-05\n",
            "epoch 987, loss 9.665876132203266e-05\n",
            "epoch 988, loss 9.662931552156806e-05\n",
            "epoch 989, loss 9.65995859587565e-05\n",
            "epoch 990, loss 9.657015471020713e-05\n",
            "epoch 991, loss 9.654039604356512e-05\n",
            "epoch 992, loss 9.651101572671905e-05\n",
            "epoch 993, loss 9.64818027568981e-05\n",
            "epoch 994, loss 9.645224781706929e-05\n",
            "epoch 995, loss 9.642302029533312e-05\n",
            "epoch 996, loss 9.63938218774274e-05\n",
            "epoch 997, loss 9.63645288720727e-05\n",
            "epoch 998, loss 9.633536683395505e-05\n",
            "epoch 999, loss 9.630613931221887e-05\n",
            "epoch 1000, loss 9.627710824133828e-05\n",
            "epoch 1001, loss 9.62479825830087e-05\n",
            "epoch 1002, loss 9.621876233723015e-05\n",
            "epoch 1003, loss 9.618989861337468e-05\n",
            "epoch 1004, loss 9.616079478291795e-05\n",
            "epoch 1005, loss 9.613192378310487e-05\n",
            "epoch 1006, loss 9.610327833797783e-05\n",
            "epoch 1007, loss 9.607426181901246e-05\n",
            "epoch 1008, loss 9.604561637388542e-05\n",
            "epoch 1009, loss 9.60167744779028e-05\n",
            "epoch 1010, loss 9.598799078958109e-05\n",
            "epoch 1011, loss 9.595945448381826e-05\n",
            "epoch 1012, loss 9.59305907599628e-05\n",
            "epoch 1013, loss 9.590197441866621e-05\n",
            "epoch 1014, loss 9.5873445388861e-05\n",
            "epoch 1015, loss 9.584480721969157e-05\n",
            "epoch 1016, loss 9.581628546584398e-05\n",
            "epoch 1017, loss 9.578782191965729e-05\n",
            "epoch 1018, loss 9.575928561389446e-05\n",
            "epoch 1019, loss 9.573103307047859e-05\n",
            "epoch 1020, loss 9.570272959535941e-05\n",
            "epoch 1021, loss 9.56743533606641e-05\n",
            "epoch 1022, loss 9.564594802213833e-05\n",
            "epoch 1023, loss 9.561786282574758e-05\n",
            "epoch 1024, loss 9.558956662658602e-05\n",
            "epoch 1025, loss 9.556146687828004e-05\n",
            "epoch 1026, loss 9.553317067911848e-05\n",
            "epoch 1027, loss 9.550513641443104e-05\n",
            "epoch 1028, loss 9.547701483825222e-05\n",
            "epoch 1029, loss 9.544902422931045e-05\n",
            "epoch 1030, loss 9.542100451653823e-05\n",
            "epoch 1031, loss 9.539309394313022e-05\n",
            "epoch 1032, loss 9.536513971397653e-05\n",
            "epoch 1033, loss 9.533722914056852e-05\n",
            "epoch 1034, loss 9.530930401524529e-05\n",
            "epoch 1035, loss 9.528161899652332e-05\n",
            "epoch 1036, loss 9.52538539422676e-05\n",
            "epoch 1037, loss 9.522611071588472e-05\n",
            "epoch 1038, loss 9.519842569716275e-05\n",
            "epoch 1039, loss 9.517078433418646e-05\n",
            "epoch 1040, loss 9.51430483837612e-05\n",
            "epoch 1041, loss 9.511545795248821e-05\n",
            "epoch 1042, loss 9.508797666057944e-05\n",
            "epoch 1043, loss 9.506036440143362e-05\n",
            "epoch 1044, loss 9.503291948931292e-05\n",
            "epoch 1045, loss 9.500548912910745e-05\n",
            "epoch 1046, loss 9.497806604485959e-05\n",
            "epoch 1047, loss 9.495060658082366e-05\n",
            "epoch 1048, loss 9.49232853599824e-05\n",
            "epoch 1049, loss 9.489605145063251e-05\n",
            "epoch 1050, loss 9.486873022979125e-05\n",
            "epoch 1051, loss 9.484152542427182e-05\n",
            "epoch 1052, loss 9.481430606683716e-05\n",
            "epoch 1053, loss 9.478710853727534e-05\n",
            "epoch 1054, loss 9.475995466345921e-05\n",
            "epoch 1055, loss 9.473282989347354e-05\n",
            "epoch 1056, loss 9.470570512348786e-05\n",
            "epoch 1057, loss 9.46786385611631e-05\n",
            "epoch 1058, loss 9.465162293054163e-05\n",
            "epoch 1059, loss 9.462446178076789e-05\n",
            "epoch 1060, loss 9.459755528951064e-05\n",
            "epoch 1061, loss 9.457061969442293e-05\n",
            "epoch 1062, loss 9.454382234252989e-05\n",
            "epoch 1063, loss 9.451687947148457e-05\n",
            "epoch 1064, loss 9.449008939554915e-05\n",
            "epoch 1065, loss 9.446316835237667e-05\n",
            "epoch 1066, loss 9.44364073802717e-05\n",
            "epoch 1067, loss 9.440976282348856e-05\n",
            "epoch 1068, loss 9.438322013011202e-05\n",
            "epoch 1069, loss 9.435647370992228e-05\n",
            "epoch 1070, loss 9.432974184164777e-05\n",
            "epoch 1071, loss 9.430309000890702e-05\n",
            "epoch 1072, loss 9.427658369531855e-05\n",
            "epoch 1073, loss 9.425007738173008e-05\n",
            "epoch 1074, loss 9.422365110367537e-05\n",
            "epoch 1075, loss 9.419705020263791e-05\n",
            "epoch 1076, loss 9.417060937266797e-05\n",
            "epoch 1077, loss 9.414403029950336e-05\n",
            "epoch 1078, loss 9.4117785920389e-05\n",
            "epoch 1079, loss 9.409133781446144e-05\n",
            "epoch 1080, loss 9.406500612385571e-05\n",
            "epoch 1081, loss 9.403869626112282e-05\n",
            "epoch 1082, loss 9.401233546668664e-05\n",
            "epoch 1083, loss 9.398609108757228e-05\n",
            "epoch 1084, loss 9.395980305271223e-05\n",
            "epoch 1085, loss 9.393350046593696e-05\n",
            "epoch 1086, loss 9.39074088819325e-05\n",
            "epoch 1087, loss 9.388123726239428e-05\n",
            "epoch 1088, loss 9.385526936966926e-05\n",
            "epoch 1089, loss 9.382895950693637e-05\n",
            "epoch 1090, loss 9.380287519888952e-05\n",
            "epoch 1091, loss 9.377684909850359e-05\n",
            "epoch 1092, loss 9.375078661832958e-05\n",
            "epoch 1093, loss 9.372478962177411e-05\n",
            "epoch 1094, loss 9.369884355692193e-05\n",
            "epoch 1095, loss 9.367294114781544e-05\n",
            "epoch 1096, loss 9.36470169108361e-05\n",
            "epoch 1097, loss 9.362108539789915e-05\n",
            "epoch 1098, loss 9.359516116091982e-05\n",
            "epoch 1099, loss 9.356946247862652e-05\n",
            "epoch 1100, loss 9.354361827718094e-05\n",
            "epoch 1101, loss 9.351778862765059e-05\n",
            "epoch 1102, loss 9.349218453280628e-05\n",
            "epoch 1103, loss 9.346639853902161e-05\n",
            "epoch 1104, loss 9.344067075289786e-05\n",
            "epoch 1105, loss 9.341495024273172e-05\n",
            "epoch 1106, loss 9.338944073533639e-05\n",
            "epoch 1107, loss 9.336385119240731e-05\n",
            "epoch 1108, loss 9.333838534075767e-05\n",
            "epoch 1109, loss 9.331294859293848e-05\n",
            "epoch 1110, loss 9.328743908554316e-05\n",
            "epoch 1111, loss 9.326208237325773e-05\n",
            "epoch 1112, loss 9.323674021288753e-05\n",
            "epoch 1113, loss 9.321139077655971e-05\n",
            "epoch 1114, loss 9.318606316810474e-05\n",
            "epoch 1115, loss 9.3160750111565e-05\n",
            "epoch 1116, loss 9.313548071077093e-05\n",
            "epoch 1117, loss 9.311024768976495e-05\n",
            "epoch 1118, loss 9.308490552939475e-05\n",
            "epoch 1119, loss 9.305991989094764e-05\n",
            "epoch 1120, loss 9.303467231802642e-05\n",
            "epoch 1121, loss 9.300962119596079e-05\n",
            "epoch 1122, loss 9.298465010942891e-05\n",
            "epoch 1123, loss 9.295951895182952e-05\n",
            "epoch 1124, loss 9.293456969317049e-05\n",
            "epoch 1125, loss 9.290946763940156e-05\n",
            "epoch 1126, loss 9.288456931244582e-05\n",
            "epoch 1127, loss 9.285975829698145e-05\n",
            "epoch 1128, loss 9.283467807108536e-05\n",
            "epoch 1129, loss 9.280984522774816e-05\n",
            "epoch 1130, loss 9.278499783249572e-05\n",
            "epoch 1131, loss 9.276011405745521e-05\n",
            "epoch 1132, loss 9.273534669773653e-05\n",
            "epoch 1133, loss 9.271065937355161e-05\n",
            "epoch 1134, loss 9.268593566957861e-05\n",
            "epoch 1135, loss 9.266134293284267e-05\n",
            "epoch 1136, loss 9.263670654036105e-05\n",
            "epoch 1137, loss 9.261204832000658e-05\n",
            "epoch 1138, loss 9.258751379093155e-05\n",
            "epoch 1139, loss 9.25630156416446e-05\n",
            "epoch 1140, loss 9.25384956644848e-05\n",
            "epoch 1141, loss 9.251394658349454e-05\n",
            "epoch 1142, loss 9.24895575735718e-05\n",
            "epoch 1143, loss 9.246507397620007e-05\n",
            "epoch 1144, loss 9.244063403457403e-05\n",
            "epoch 1145, loss 9.241617954103276e-05\n",
            "epoch 1146, loss 9.2391885118559e-05\n",
            "epoch 1147, loss 9.236777987098321e-05\n",
            "epoch 1148, loss 9.234347089659423e-05\n",
            "epoch 1149, loss 9.231930744135752e-05\n",
            "epoch 1150, loss 9.229520219378173e-05\n",
            "epoch 1151, loss 9.227111149812117e-05\n",
            "epoch 1152, loss 9.22469625947997e-05\n",
            "epoch 1153, loss 9.222275548381731e-05\n",
            "epoch 1154, loss 9.21987957553938e-05\n",
            "epoch 1155, loss 9.217477781930938e-05\n",
            "epoch 1156, loss 9.215089812641963e-05\n",
            "epoch 1157, loss 9.212691657012329e-05\n",
            "epoch 1158, loss 9.210290590999648e-05\n",
            "epoch 1159, loss 9.207888797391206e-05\n",
            "epoch 1160, loss 9.20549500733614e-05\n",
            "epoch 1161, loss 9.203111403621733e-05\n",
            "epoch 1162, loss 9.200724889524281e-05\n",
            "epoch 1163, loss 9.198345651384443e-05\n",
            "epoch 1164, loss 9.195962047670037e-05\n",
            "epoch 1165, loss 9.193579899147153e-05\n",
            "epoch 1166, loss 9.19120357139036e-05\n",
            "epoch 1167, loss 9.18881778488867e-05\n",
            "epoch 1168, loss 9.186456736642867e-05\n",
            "epoch 1169, loss 9.184096415992826e-05\n",
            "epoch 1170, loss 9.181709901895374e-05\n",
            "epoch 1171, loss 9.179356857202947e-05\n",
            "epoch 1172, loss 9.176989260595292e-05\n",
            "epoch 1173, loss 9.17463912628591e-05\n",
            "epoch 1174, loss 9.17226861929521e-05\n",
            "epoch 1175, loss 9.169908298645169e-05\n",
            "epoch 1176, loss 9.167563985101879e-05\n",
            "epoch 1177, loss 9.165214578388259e-05\n",
            "epoch 1178, loss 9.162874630419537e-05\n",
            "epoch 1179, loss 9.160520130535588e-05\n",
            "epoch 1180, loss 9.158188186120242e-05\n",
            "epoch 1181, loss 9.155862062470987e-05\n",
            "epoch 1182, loss 9.15352429728955e-05\n",
            "epoch 1183, loss 9.151201084023342e-05\n",
            "epoch 1184, loss 9.148874232778326e-05\n",
            "epoch 1185, loss 9.146543015958741e-05\n",
            "epoch 1186, loss 9.144212526734918e-05\n",
            "epoch 1187, loss 9.1419045929797e-05\n",
            "epoch 1188, loss 9.139579924521968e-05\n",
            "epoch 1189, loss 9.137285087490454e-05\n",
            "epoch 1190, loss 9.134972788160667e-05\n",
            "epoch 1191, loss 9.13268158910796e-05\n",
            "epoch 1192, loss 9.130399121204391e-05\n",
            "epoch 1193, loss 9.128078090725467e-05\n",
            "epoch 1194, loss 9.12579198484309e-05\n",
            "epoch 1195, loss 9.123502968577668e-05\n",
            "epoch 1196, loss 9.121224866248667e-05\n",
            "epoch 1197, loss 9.118944581132382e-05\n",
            "epoch 1198, loss 9.116666478803381e-05\n",
            "epoch 1199, loss 9.114386921282858e-05\n",
            "epoch 1200, loss 9.112102998187765e-05\n",
            "epoch 1201, loss 9.109819075092673e-05\n",
            "epoch 1202, loss 9.107540972763672e-05\n",
            "epoch 1203, loss 9.105265053221956e-05\n",
            "epoch 1204, loss 9.102994226850569e-05\n",
            "epoch 1205, loss 9.100721217691898e-05\n",
            "epoch 1206, loss 9.098452574107796e-05\n",
            "epoch 1207, loss 9.096185385715216e-05\n",
            "epoch 1208, loss 9.093929111259058e-05\n",
            "epoch 1209, loss 9.09165755729191e-05\n",
            "epoch 1210, loss 9.089386730920523e-05\n",
            "epoch 1211, loss 9.087146463571116e-05\n",
            "epoch 1212, loss 9.084893099498004e-05\n",
            "epoch 1213, loss 9.08264410099946e-05\n",
            "epoch 1214, loss 9.08038200577721e-05\n",
            "epoch 1215, loss 9.078149014385417e-05\n",
            "epoch 1216, loss 9.075905109057203e-05\n",
            "epoch 1217, loss 9.073645196622238e-05\n",
            "epoch 1218, loss 9.071426029549912e-05\n",
            "epoch 1219, loss 9.069201769307256e-05\n",
            "epoch 1220, loss 9.066965867532417e-05\n",
            "epoch 1221, loss 9.064748883247375e-05\n",
            "epoch 1222, loss 9.06252971617505e-05\n",
            "epoch 1223, loss 9.060307638719678e-05\n",
            "epoch 1224, loss 9.058090654434636e-05\n",
            "epoch 1225, loss 9.055872214958072e-05\n",
            "epoch 1226, loss 9.053654503077269e-05\n",
            "epoch 1227, loss 9.051446977537125e-05\n",
            "epoch 1228, loss 9.049232176039368e-05\n",
            "epoch 1229, loss 9.047023922903463e-05\n",
            "epoch 1230, loss 9.044804028235376e-05\n",
            "epoch 1231, loss 9.042603778652847e-05\n",
            "epoch 1232, loss 9.040407894644886e-05\n",
            "epoch 1233, loss 9.038212738232687e-05\n",
            "epoch 1234, loss 9.036019037012011e-05\n",
            "epoch 1235, loss 9.033836977323517e-05\n",
            "epoch 1236, loss 9.03164764167741e-05\n",
            "epoch 1237, loss 9.029469219967723e-05\n",
            "epoch 1238, loss 9.027272608364001e-05\n",
            "epoch 1239, loss 9.025095641845837e-05\n",
            "epoch 1240, loss 9.022912126965821e-05\n",
            "epoch 1241, loss 9.020747529575601e-05\n",
            "epoch 1242, loss 9.018558193929493e-05\n",
            "epoch 1243, loss 9.01639650692232e-05\n",
            "epoch 1244, loss 9.014219540404156e-05\n",
            "epoch 1245, loss 9.01205203263089e-05\n",
            "epoch 1246, loss 9.009885980049148e-05\n",
            "epoch 1247, loss 9.007711196318269e-05\n",
            "epoch 1248, loss 9.005561878439039e-05\n",
            "epoch 1249, loss 9.003406012197956e-05\n",
            "epoch 1250, loss 9.001269063446671e-05\n",
            "epoch 1251, loss 8.999111014418304e-05\n",
            "epoch 1252, loss 8.99695442058146e-05\n",
            "epoch 1253, loss 8.994823292596266e-05\n",
            "epoch 1254, loss 8.99266597116366e-05\n",
            "epoch 1255, loss 8.990520291263238e-05\n",
            "epoch 1256, loss 8.988380432128906e-05\n",
            "epoch 1257, loss 8.986236935015768e-05\n",
            "epoch 1258, loss 8.984106534626335e-05\n",
            "epoch 1259, loss 8.981972496258095e-05\n",
            "epoch 1260, loss 8.97982536116615e-05\n",
            "epoch 1261, loss 8.977700053947046e-05\n",
            "epoch 1262, loss 8.975572563940659e-05\n",
            "epoch 1263, loss 8.973445801530033e-05\n",
            "epoch 1264, loss 8.971312490757555e-05\n",
            "epoch 1265, loss 8.969172631623223e-05\n",
            "epoch 1266, loss 8.96705751074478e-05\n",
            "epoch 1267, loss 8.964935113908723e-05\n",
            "epoch 1268, loss 8.96280980668962e-05\n",
            "epoch 1269, loss 8.960691775428131e-05\n",
            "epoch 1270, loss 8.958568650996312e-05\n",
            "epoch 1271, loss 8.95644843694754e-05\n",
            "epoch 1272, loss 8.954336226452142e-05\n",
            "epoch 1273, loss 8.95222183316946e-05\n",
            "epoch 1274, loss 8.95010816748254e-05\n",
            "epoch 1275, loss 8.948008326115087e-05\n",
            "epoch 1276, loss 8.945913577917963e-05\n",
            "epoch 1277, loss 8.943799912231043e-05\n",
            "epoch 1278, loss 8.941693522501737e-05\n",
            "epoch 1279, loss 8.939604595070705e-05\n",
            "epoch 1280, loss 8.937516395235434e-05\n",
            "epoch 1281, loss 8.935421647038311e-05\n",
            "epoch 1282, loss 8.933334174798802e-05\n",
            "epoch 1283, loss 8.931245974963531e-05\n",
            "epoch 1284, loss 8.929157047532499e-05\n",
            "epoch 1285, loss 8.927065937314183e-05\n",
            "epoch 1286, loss 8.924980647861958e-05\n",
            "epoch 1287, loss 8.922890992835164e-05\n",
            "epoch 1288, loss 8.9208064309787e-05\n",
            "epoch 1289, loss 8.91872841748409e-05\n",
            "epoch 1290, loss 8.916650403989479e-05\n",
            "epoch 1291, loss 8.914581121644005e-05\n",
            "epoch 1292, loss 8.912505290936679e-05\n",
            "epoch 1293, loss 8.910434553399682e-05\n",
            "epoch 1294, loss 8.908379095373675e-05\n",
            "epoch 1295, loss 8.906316361390054e-05\n",
            "epoch 1296, loss 8.904269634513184e-05\n",
            "epoch 1297, loss 8.902228728402406e-05\n",
            "epoch 1298, loss 8.900175453163683e-05\n",
            "epoch 1299, loss 8.898133091861382e-05\n",
            "epoch 1300, loss 8.89608490979299e-05\n",
            "epoch 1301, loss 8.894057828001678e-05\n",
            "epoch 1302, loss 8.89201182872057e-05\n",
            "epoch 1303, loss 8.88997528818436e-05\n",
            "epoch 1304, loss 8.887927833711728e-05\n",
            "epoch 1305, loss 8.885910938261077e-05\n",
            "epoch 1306, loss 8.883872214937583e-05\n",
            "epoch 1307, loss 8.881838584784418e-05\n",
            "epoch 1308, loss 8.879810775397345e-05\n",
            "epoch 1309, loss 8.877765503711998e-05\n",
            "epoch 1310, loss 8.875758067006245e-05\n",
            "epoch 1311, loss 8.87373389559798e-05\n",
            "epoch 1312, loss 8.871725731296465e-05\n",
            "epoch 1313, loss 8.86971756699495e-05\n",
            "epoch 1314, loss 8.867712313076481e-05\n",
            "epoch 1315, loss 8.865697600413114e-05\n",
            "epoch 1316, loss 8.863706898409873e-05\n",
            "epoch 1317, loss 8.861675451043993e-05\n",
            "epoch 1318, loss 8.859682566253468e-05\n",
            "epoch 1319, loss 8.857680222718045e-05\n",
            "epoch 1320, loss 8.855673513608053e-05\n",
            "epoch 1321, loss 8.853675535647199e-05\n",
            "epoch 1322, loss 8.851687016431242e-05\n",
            "epoch 1323, loss 8.84969049366191e-05\n",
            "epoch 1324, loss 8.847702702041715e-05\n",
            "epoch 1325, loss 8.845709089655429e-05\n",
            "epoch 1326, loss 8.843726391205564e-05\n",
            "epoch 1327, loss 8.841748785926029e-05\n",
            "epoch 1328, loss 8.839751535560936e-05\n",
            "epoch 1329, loss 8.837779751047492e-05\n",
            "epoch 1330, loss 8.835791959427297e-05\n",
            "epoch 1331, loss 8.833807805785909e-05\n",
            "epoch 1332, loss 8.831818558974192e-05\n",
            "epoch 1333, loss 8.829843864077702e-05\n",
            "epoch 1334, loss 8.827861893223599e-05\n",
            "epoch 1335, loss 8.825895201880485e-05\n",
            "epoch 1336, loss 8.823905227473006e-05\n",
            "epoch 1337, loss 8.821940718917176e-05\n",
            "epoch 1338, loss 8.819956565275788e-05\n",
            "epoch 1339, loss 8.817999332677573e-05\n",
            "epoch 1340, loss 8.816046465653926e-05\n",
            "epoch 1341, loss 8.814060129225254e-05\n",
            "epoch 1342, loss 8.812105807010084e-05\n",
            "epoch 1343, loss 8.810155122773722e-05\n",
            "epoch 1344, loss 8.808201528154314e-05\n",
            "epoch 1345, loss 8.80623992998153e-05\n",
            "epoch 1346, loss 8.804297249298543e-05\n",
            "epoch 1347, loss 8.802343654679134e-05\n",
            "epoch 1348, loss 8.800396608421579e-05\n",
            "epoch 1349, loss 8.79845320014283e-05\n",
            "epoch 1350, loss 8.796515612630174e-05\n",
            "epoch 1351, loss 8.794568566372618e-05\n",
            "epoch 1352, loss 8.79262515809387e-05\n",
            "epoch 1353, loss 8.790687570581213e-05\n",
            "epoch 1354, loss 8.788743434706703e-05\n",
            "epoch 1355, loss 8.786803664406762e-05\n",
            "epoch 1356, loss 8.784864621702582e-05\n",
            "epoch 1357, loss 8.78292994457297e-05\n",
            "epoch 1358, loss 8.78099090186879e-05\n",
            "epoch 1359, loss 8.77905185916461e-05\n",
            "epoch 1360, loss 8.77713828231208e-05\n",
            "epoch 1361, loss 8.775221067480743e-05\n",
            "epoch 1362, loss 8.773303852649406e-05\n",
            "epoch 1363, loss 8.771374268690124e-05\n",
            "epoch 1364, loss 8.769460691837594e-05\n",
            "epoch 1365, loss 8.767549297772348e-05\n",
            "epoch 1366, loss 8.76563208294101e-05\n",
            "epoch 1367, loss 8.763714868109673e-05\n",
            "epoch 1368, loss 8.76181002240628e-05\n",
            "epoch 1369, loss 8.759894990362227e-05\n",
            "epoch 1370, loss 8.757985051488504e-05\n",
            "epoch 1371, loss 8.756070747040212e-05\n",
            "epoch 1372, loss 8.754153532208875e-05\n",
            "epoch 1373, loss 8.752249414101243e-05\n",
            "epoch 1374, loss 8.750338747631758e-05\n",
            "epoch 1375, loss 8.748439722694457e-05\n",
            "epoch 1376, loss 8.746522507863119e-05\n",
            "epoch 1377, loss 8.74465040396899e-05\n",
            "epoch 1378, loss 8.742773934500292e-05\n",
            "epoch 1379, loss 8.740882913116366e-05\n",
            "epoch 1380, loss 8.738996257307008e-05\n",
            "epoch 1381, loss 8.737117605051026e-05\n",
            "epoch 1382, loss 8.735236042411998e-05\n",
            "epoch 1383, loss 8.733351569389924e-05\n",
            "epoch 1384, loss 8.731479465495795e-05\n",
            "epoch 1385, loss 8.72959935804829e-05\n",
            "epoch 1386, loss 8.72771124704741e-05\n",
            "epoch 1387, loss 8.725841325940564e-05\n",
            "epoch 1388, loss 8.723981591174379e-05\n",
            "epoch 1389, loss 8.722102938918397e-05\n",
            "epoch 1390, loss 8.720225014258176e-05\n",
            "epoch 1391, loss 8.718358731130138e-05\n",
            "epoch 1392, loss 8.716495358385146e-05\n",
            "epoch 1393, loss 8.714631985640153e-05\n",
            "epoch 1394, loss 8.712770795682445e-05\n",
            "epoch 1395, loss 8.710939437150955e-05\n",
            "epoch 1396, loss 8.709091343916953e-05\n",
            "epoch 1397, loss 8.707246888661757e-05\n",
            "epoch 1398, loss 8.705393702257425e-05\n",
            "epoch 1399, loss 8.703556522959843e-05\n",
            "epoch 1400, loss 8.701719343662262e-05\n",
            "epoch 1401, loss 8.699876343598589e-05\n",
            "epoch 1402, loss 8.698036981513724e-05\n",
            "epoch 1403, loss 8.69620344019495e-05\n",
            "epoch 1404, loss 8.694358984939754e-05\n",
            "epoch 1405, loss 8.69251525728032e-05\n",
            "epoch 1406, loss 8.690691174706444e-05\n",
            "epoch 1407, loss 8.688859088579193e-05\n",
            "epoch 1408, loss 8.687018998898566e-05\n",
            "epoch 1409, loss 8.685192733537406e-05\n",
            "epoch 1410, loss 8.683365740580484e-05\n",
            "epoch 1411, loss 8.681538747623563e-05\n",
            "epoch 1412, loss 8.679718303028494e-05\n",
            "epoch 1413, loss 8.677897130837664e-05\n",
            "epoch 1414, loss 8.676081051817164e-05\n",
            "epoch 1415, loss 8.674265700392425e-05\n",
            "epoch 1416, loss 8.672435797052458e-05\n",
            "epoch 1417, loss 8.670644456287846e-05\n",
            "epoch 1418, loss 8.668818190926686e-05\n",
            "epoch 1419, loss 8.667007932672277e-05\n",
            "epoch 1420, loss 8.665202039992437e-05\n",
            "epoch 1421, loss 8.663388871354982e-05\n",
            "epoch 1422, loss 8.661562605993822e-05\n",
            "epoch 1423, loss 8.659763989271596e-05\n",
            "epoch 1424, loss 8.65795518620871e-05\n",
            "epoch 1425, loss 8.656160935061052e-05\n",
            "epoch 1426, loss 8.654349949210882e-05\n",
            "epoch 1427, loss 8.652565156808123e-05\n",
            "epoch 1428, loss 8.650773816043511e-05\n",
            "epoch 1429, loss 8.648969378555194e-05\n",
            "epoch 1430, loss 8.647178765386343e-05\n",
            "epoch 1431, loss 8.64540197653696e-05\n",
            "epoch 1432, loss 8.643601177027449e-05\n",
            "epoch 1433, loss 8.641827298561111e-05\n",
            "epoch 1434, loss 8.64004177856259e-05\n",
            "epoch 1435, loss 8.638259896542877e-05\n",
            "epoch 1436, loss 8.636478742118925e-05\n",
            "epoch 1437, loss 8.634699042886496e-05\n",
            "epoch 1438, loss 8.63291643327102e-05\n",
            "epoch 1439, loss 8.631133096059784e-05\n",
            "epoch 1440, loss 8.629352669231594e-05\n",
            "epoch 1441, loss 8.627577335573733e-05\n",
            "epoch 1442, loss 8.625803457107395e-05\n",
            "epoch 1443, loss 8.624023030279204e-05\n",
            "epoch 1444, loss 8.622255700174719e-05\n",
            "epoch 1445, loss 8.620473818155006e-05\n",
            "epoch 1446, loss 8.618698484497145e-05\n",
            "epoch 1447, loss 8.61693115439266e-05\n",
            "epoch 1448, loss 8.61516673467122e-05\n",
            "epoch 1449, loss 8.613415411673486e-05\n",
            "epoch 1450, loss 8.611658267909661e-05\n",
            "epoch 1451, loss 8.609909127699211e-05\n",
            "epoch 1452, loss 8.608157804701477e-05\n",
            "epoch 1453, loss 8.606408664491028e-05\n",
            "epoch 1454, loss 8.604650793131441e-05\n",
            "epoch 1455, loss 8.602883463026956e-05\n",
            "epoch 1456, loss 8.601134322816506e-05\n",
            "epoch 1457, loss 8.599368447903544e-05\n",
            "epoch 1458, loss 8.597624400863424e-05\n",
            "epoch 1459, loss 8.59587307786569e-05\n",
            "epoch 1460, loss 8.594117389293388e-05\n",
            "epoch 1461, loss 8.592368249082938e-05\n",
            "epoch 1462, loss 8.590607467340305e-05\n",
            "epoch 1463, loss 8.588864875491709e-05\n",
            "epoch 1464, loss 8.587122283643112e-05\n",
            "epoch 1465, loss 8.585361501900479e-05\n",
            "epoch 1466, loss 8.583621820434928e-05\n",
            "epoch 1467, loss 8.581891597714275e-05\n",
            "epoch 1468, loss 8.580136636737734e-05\n",
            "epoch 1469, loss 8.578421693528071e-05\n",
            "epoch 1470, loss 8.576691470807418e-05\n",
            "epoch 1471, loss 8.574959792895243e-05\n",
            "epoch 1472, loss 8.573232480557635e-05\n",
            "epoch 1473, loss 8.571509533794597e-05\n",
            "epoch 1474, loss 8.56978222145699e-05\n",
            "epoch 1475, loss 8.568054909119383e-05\n",
            "epoch 1476, loss 8.566337055526674e-05\n",
            "epoch 1477, loss 8.564617019146681e-05\n",
            "epoch 1478, loss 8.562890434404835e-05\n",
            "epoch 1479, loss 8.561159484088421e-05\n",
            "epoch 1480, loss 8.559448178857565e-05\n",
            "epoch 1481, loss 8.557740511605516e-05\n",
            "epoch 1482, loss 8.556020475225523e-05\n",
            "epoch 1483, loss 8.554298256058246e-05\n",
            "epoch 1484, loss 8.552584768040106e-05\n",
            "epoch 1485, loss 8.550875645596534e-05\n",
            "epoch 1486, loss 8.549162885174155e-05\n",
            "epoch 1487, loss 8.547466859454289e-05\n",
            "epoch 1488, loss 8.545754826627672e-05\n",
            "epoch 1489, loss 8.544054435333237e-05\n",
            "epoch 1490, loss 8.542349678464234e-05\n",
            "epoch 1491, loss 8.540653652744368e-05\n",
            "epoch 1492, loss 8.538949623471126e-05\n",
            "epoch 1493, loss 8.537254325347021e-05\n",
            "epoch 1494, loss 8.535541564924642e-05\n",
            "epoch 1495, loss 8.533855725545436e-05\n",
            "epoch 1496, loss 8.532172796549276e-05\n",
            "epoch 1497, loss 8.530452760169283e-05\n",
            "epoch 1498, loss 8.528756734449416e-05\n",
            "epoch 1499, loss 8.527059981133789e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_7Q7S7TRQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSwP0e7JTiBx"
      },
      "source": [
        "predicted = model(X_train).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tg6L4Gc-TiFJ",
        "outputId": "c8bec235-dff4-4455-87be-80d600e52a77"
      },
      "source": [
        "plt.scatter(X_train.detach().numpy()[:100] , Y_train.detach().numpy()[:100])\n",
        "plt.plot(X_train.detach().numpy()[:100] , predicted[:100] , \"red\")\n",
        "plt.xlabel(\"Celcius\")\n",
        "plt.ylabel(\"Farenhite\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F-N8b58Tpkk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnW2jLEvbpJ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpt3SUeRbpM4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "I8FS55YmaMDL",
        "outputId": "8a3dc94f-e67b-465d-9c5b-d5420f01db39"
      },
      "source": [
        "test=pd.read_csv(\"/content/drive/MyDrive/dataset/Celsius to Fahrenheit/testing.csv\")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Celsius</th>\n",
              "      <th>Fahrenheit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>259</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2351</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2112</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2239</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1016</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Celsius  Fahrenheit\n",
              "0      259         NaN\n",
              "1     2351         NaN\n",
              "2     2112         NaN\n",
              "3     2239         NaN\n",
              "4     1016         NaN"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUXkTkWZaMGF",
        "outputId": "1c6fc564-ee33-4ad7-add4-e04aaca60a75"
      },
      "source": [
        "test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Celsius          0\n",
              "Fahrenheit    8000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WftAzg9radDB",
        "outputId": "45a188fe-d943-4e67-f9a2-92fa9eab3a7d"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY28kpbsadGR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4-i8crKY-CH"
      },
      "source": [
        "x=torch.Tensor([2380])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJeZsljKY8yR"
      },
      "source": [
        "predicted2 = model(x).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNLNORxjZEly",
        "outputId": "5dc0ef88-932a-498d-9862-083cafe57f8d"
      },
      "source": [
        "predicted2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([230.19376], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZAO2zP3ZZwS",
        "outputId": "d0b3fc08-18ed-4547-ef93-c56754ae72ae"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4676],\n",
              "        [0.5120],\n",
              "        [0.9520],\n",
              "        ...,\n",
              "        [0.6336],\n",
              "        [0.9584],\n",
              "        [0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPzmdumxZPnc",
        "outputId": "412d7a58-43d8-4fdc-e92d-c62536ec61b3"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4676],\n",
              "        [0.5120],\n",
              "        [0.9520],\n",
              "        ...,\n",
              "        [0.6336],\n",
              "        [0.9584],\n",
              "        [0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjtAbku9ZWVn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}