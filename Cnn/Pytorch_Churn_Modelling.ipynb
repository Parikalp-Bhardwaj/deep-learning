{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Churn_Modelling.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
       "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
       "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['RowNumber', 'CustomerId', 'Surname','Geography','Gender', 'Age'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop([\"Exited\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CreditScore', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
       "       'IsActiveMember', 'EstimatedSalary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "x[[\"CreditScore\",\"EstimatedSalary\"]]=scaler.fit_transform(x[[\"CreditScore\",\"EstimatedSalary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data[\"Exited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array(x,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x1.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.538     ],\n",
       "       [2.        ],\n",
       "       [0.        ],\n",
       "       ...,\n",
       "       [1.        ],\n",
       "       [0.        ],\n",
       "       [0.19091423]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=np.array(y,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 1), (10000,))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape,y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=x.values\n",
    "y2=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=torch.from_numpy(x2)\n",
    "y2=torch.from_numpy(y2).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5380, 2.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.5067],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,x_test,Y_train,y_test=train_test_split(x2,y2,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train)\n",
    "# y_train = torch.from_numpy(y_train).view(-1,1)\n",
    "\n",
    "# # Numpy to Tensor Conversion (Train Set)\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# y_test = torch.from_numpy(y_test).view(-1,1)\n",
    "\n",
    "# kaggle_test_set = torch.from_numpy(kaggle_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim = 11, output_dim = 1):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "    \n",
    "        # Input Layer (784) -> 784\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        # 64 -> 64\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # 64 -> 32\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        # 32 -> 32\n",
    "        self.fc4 = nn.Linear(32, 32)\n",
    "        # 32 -> output layer(10)\n",
    "        self.output_layer = nn.Linear(32,1)\n",
    "        # Dropout Layer (20%) to reduce overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    # Feed Forward Function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Add ReLU activation function to each layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        # Return the created model\n",
    "#         return F.softmax(x,dim=1)\n",
    "        return nn.Sigmoid()(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 7\n",
    "output_dim=1\n",
    "model=LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9,nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x00000131BF9B0548>\n",
      "torch.Size([64, 7])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "print(list(model.parameters())[1].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train,Y_train)\n",
    "test = torch.utils.data.TensorDataset(x_test,y_test)\n",
    "\n",
    "# Create train and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.2781\t Acc: 71.00%\n",
      "Epoch: 2 \tTraining Loss: 0.1963\t Acc: 79.00%\n",
      "Epoch: 3 \tTraining Loss: 0.1958\t Acc: 79.00%\n",
      "Epoch: 4 \tTraining Loss: 0.1955\t Acc: 79.00%\n",
      "Epoch: 5 \tTraining Loss: 0.1954\t Acc: 79.00%\n",
      "Epoch: 6 \tTraining Loss: 0.1952\t Acc: 79.00%\n",
      "Epoch: 7 \tTraining Loss: 0.1951\t Acc: 79.00%\n",
      "Epoch: 8 \tTraining Loss: 0.1951\t Acc: 79.00%\n",
      "Epoch: 9 \tTraining Loss: 0.1948\t Acc: 79.00%\n",
      "Epoch: 10 \tTraining Loss: 0.1949\t Acc: 79.00%\n",
      "Epoch: 11 \tTraining Loss: 0.1945\t Acc: 79.00%\n",
      "Epoch: 12 \tTraining Loss: 0.1943\t Acc: 79.00%\n",
      "Epoch: 13 \tTraining Loss: 0.1943\t Acc: 79.00%\n",
      "Epoch: 14 \tTraining Loss: 0.1941\t Acc: 79.00%\n",
      "Epoch: 15 \tTraining Loss: 0.1939\t Acc: 79.00%\n",
      "Epoch: 16 \tTraining Loss: 0.1937\t Acc: 79.00%\n",
      "Epoch: 17 \tTraining Loss: 0.1936\t Acc: 79.00%\n",
      "Epoch: 18 \tTraining Loss: 0.1933\t Acc: 79.00%\n",
      "Epoch: 19 \tTraining Loss: 0.1931\t Acc: 79.00%\n",
      "Epoch: 20 \tTraining Loss: 0.1931\t Acc: 79.00%\n",
      "Epoch: 21 \tTraining Loss: 0.1928\t Acc: 79.00%\n",
      "Epoch: 22 \tTraining Loss: 0.1927\t Acc: 79.00%\n",
      "Epoch: 23 \tTraining Loss: 0.1925\t Acc: 79.00%\n",
      "Epoch: 24 \tTraining Loss: 0.1922\t Acc: 79.00%\n",
      "Epoch: 25 \tTraining Loss: 0.1919\t Acc: 79.00%\n",
      "Epoch: 26 \tTraining Loss: 0.1918\t Acc: 79.00%\n",
      "Epoch: 27 \tTraining Loss: 0.1914\t Acc: 79.00%\n",
      "Epoch: 28 \tTraining Loss: 0.1913\t Acc: 79.00%\n",
      "Epoch: 29 \tTraining Loss: 0.1909\t Acc: 79.00%\n",
      "Epoch: 30 \tTraining Loss: 0.1907\t Acc: 79.00%\n",
      "Epoch: 31 \tTraining Loss: 0.1906\t Acc: 79.00%\n",
      "Epoch: 32 \tTraining Loss: 0.1904\t Acc: 79.00%\n",
      "Epoch: 33 \tTraining Loss: 0.1902\t Acc: 79.00%\n",
      "Epoch: 34 \tTraining Loss: 0.1900\t Acc: 79.00%\n",
      "Epoch: 35 \tTraining Loss: 0.1901\t Acc: 79.00%\n",
      "Epoch: 36 \tTraining Loss: 0.1895\t Acc: 79.00%\n",
      "Epoch: 37 \tTraining Loss: 0.1896\t Acc: 79.00%\n",
      "Epoch: 38 \tTraining Loss: 0.1898\t Acc: 79.00%\n",
      "Epoch: 39 \tTraining Loss: 0.1896\t Acc: 79.00%\n",
      "Epoch: 40 \tTraining Loss: 0.1893\t Acc: 79.00%\n",
      "Epoch: 41 \tTraining Loss: 0.1892\t Acc: 79.00%\n",
      "Epoch: 42 \tTraining Loss: 0.1894\t Acc: 79.00%\n",
      "Epoch: 43 \tTraining Loss: 0.1894\t Acc: 79.00%\n",
      "Epoch: 44 \tTraining Loss: 0.1892\t Acc: 79.00%\n",
      "Epoch: 45 \tTraining Loss: 0.1893\t Acc: 79.00%\n",
      "Epoch: 46 \tTraining Loss: 0.1891\t Acc: 79.00%\n",
      "Epoch: 47 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 48 \tTraining Loss: 0.1892\t Acc: 79.00%\n",
      "Epoch: 49 \tTraining Loss: 0.1889\t Acc: 79.00%\n",
      "Epoch: 50 \tTraining Loss: 0.1889\t Acc: 79.00%\n",
      "Epoch: 51 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 52 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 53 \tTraining Loss: 0.1890\t Acc: 79.00%\n",
      "Epoch: 54 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 55 \tTraining Loss: 0.1891\t Acc: 79.00%\n",
      "Epoch: 56 \tTraining Loss: 0.1888\t Acc: 79.00%\n",
      "Epoch: 57 \tTraining Loss: 0.1888\t Acc: 79.00%\n",
      "Epoch: 58 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 59 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 60 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 61 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 62 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 63 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 64 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 65 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 66 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 67 \tTraining Loss: 0.1887\t Acc: 79.00%\n",
      "Epoch: 68 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 69 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 70 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 71 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 72 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 73 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 74 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 75 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 76 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 77 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 78 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 79 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 80 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 81 \tTraining Loss: 0.1886\t Acc: 79.00%\n",
      "Epoch: 82 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 83 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 84 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 85 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 86 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 87 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 88 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 89 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 90 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 91 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 92 \tTraining Loss: 0.1884\t Acc: 78.00%\n",
      "Epoch: 93 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 94 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 95 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 96 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 97 \tTraining Loss: 0.1883\t Acc: 78.00%\n",
      "Epoch: 98 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 99 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 100 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 101 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 102 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 103 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 104 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 105 \tTraining Loss: 0.1884\t Acc: 78.00%\n",
      "Epoch: 106 \tTraining Loss: 0.1886\t Acc: 78.00%\n",
      "Epoch: 107 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 108 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 109 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 110 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 111 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 112 \tTraining Loss: 0.1885\t Acc: 78.00%\n",
      "Epoch: 113 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 114 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 115 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 116 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 117 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 118 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 119 \tTraining Loss: 0.1885\t Acc: 79.00%\n",
      "Epoch: 120 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 121 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 122 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 123 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 124 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 125 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 126 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 127 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 128 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 129 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 130 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 131 \tTraining Loss: 0.1881\t Acc: 77.00%\n",
      "Epoch: 132 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 133 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 134 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 135 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 136 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 137 \tTraining Loss: 0.1880\t Acc: 77.00%\n",
      "Epoch: 138 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 139 \tTraining Loss: 0.1884\t Acc: 79.00%\n",
      "Epoch: 140 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 141 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 142 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 143 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 144 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 145 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 146 \tTraining Loss: 0.1883\t Acc: 79.00%\n",
      "Epoch: 147 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 148 \tTraining Loss: 0.1880\t Acc: 77.00%\n",
      "Epoch: 149 \tTraining Loss: 0.1883\t Acc: 78.00%\n",
      "Epoch: 150 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 151 \tTraining Loss: 0.1882\t Acc: 79.00%\n",
      "Epoch: 152 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 153 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 154 \tTraining Loss: 0.1883\t Acc: 77.00%\n",
      "Epoch: 155 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 156 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 157 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 158 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 159 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 160 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 161 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 162 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 163 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 164 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 165 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 166 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 167 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 168 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 169 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 170 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 171 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 172 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 173 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 174 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 175 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 176 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 177 \tTraining Loss: 0.1878\t Acc: 77.00%\n",
      "Epoch: 178 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 179 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 180 \tTraining Loss: 0.1879\t Acc: 77.00%\n",
      "Epoch: 181 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 182 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 183 \tTraining Loss: 0.1882\t Acc: 78.00%\n",
      "Epoch: 184 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 185 \tTraining Loss: 0.1880\t Acc: 77.00%\n",
      "Epoch: 186 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 187 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 188 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 189 \tTraining Loss: 0.1881\t Acc: 78.00%\n",
      "Epoch: 190 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 191 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 192 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 193 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 194 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 195 \tTraining Loss: 0.1879\t Acc: 76.00%\n",
      "Epoch: 196 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 197 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 198 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 199 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 200 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 201 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 202 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 203 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 204 \tTraining Loss: 0.1879\t Acc: 77.00%\n",
      "Epoch: 205 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 206 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 207 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 208 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 209 \tTraining Loss: 0.1876\t Acc: 76.00%\n",
      "Epoch: 210 \tTraining Loss: 0.1879\t Acc: 77.00%\n",
      "Epoch: 211 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 212 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 213 \tTraining Loss: 0.1880\t Acc: 79.00%\n",
      "Epoch: 214 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 215 \tTraining Loss: 0.1881\t Acc: 79.00%\n",
      "Epoch: 216 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 217 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 218 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 219 \tTraining Loss: 0.1879\t Acc: 77.00%\n",
      "Epoch: 220 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 221 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 222 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 223 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 224 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 225 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 226 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 227 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 228 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 229 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 230 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 231 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 232 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 233 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 234 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 235 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 236 \tTraining Loss: 0.1878\t Acc: 77.00%\n",
      "Epoch: 237 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 238 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 239 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 240 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 241 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 242 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 243 \tTraining Loss: 0.1878\t Acc: 77.00%\n",
      "Epoch: 244 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 245 \tTraining Loss: 0.1879\t Acc: 78.00%\n",
      "Epoch: 246 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 247 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 248 \tTraining Loss: 0.1876\t Acc: 77.00%\n",
      "Epoch: 249 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 250 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 251 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 252 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 253 \tTraining Loss: 0.1876\t Acc: 77.00%\n",
      "Epoch: 254 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 255 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 256 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 257 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 258 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 259 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 260 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 261 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 262 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 263 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 264 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 265 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 266 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 267 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 268 \tTraining Loss: 0.1877\t Acc: 77.00%\n",
      "Epoch: 269 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 270 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 271 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 272 \tTraining Loss: 0.1876\t Acc: 77.00%\n",
      "Epoch: 273 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 274 \tTraining Loss: 0.1877\t Acc: 77.00%\n",
      "Epoch: 275 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 276 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 277 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 278 \tTraining Loss: 0.1878\t Acc: 79.00%\n",
      "Epoch: 279 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 280 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 281 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 282 \tTraining Loss: 0.1880\t Acc: 78.00%\n",
      "Epoch: 283 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 284 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 285 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 286 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 287 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 288 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 289 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 290 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 291 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 292 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 293 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 294 \tTraining Loss: 0.1874\t Acc: 77.00%\n",
      "Epoch: 295 \tTraining Loss: 0.1879\t Acc: 79.00%\n",
      "Epoch: 296 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 297 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 298 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 299 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 300 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 301 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 302 \tTraining Loss: 0.1875\t Acc: 76.00%\n",
      "Epoch: 303 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 304 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 305 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 306 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 307 \tTraining Loss: 0.1878\t Acc: 77.00%\n",
      "Epoch: 308 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 309 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 310 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 311 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 312 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 313 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 314 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 315 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 316 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 317 \tTraining Loss: 0.1877\t Acc: 77.00%\n",
      "Epoch: 318 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 319 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 320 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 321 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 322 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 323 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 324 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 325 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 326 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 327 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 328 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 329 \tTraining Loss: 0.1875\t Acc: 77.00%\n",
      "Epoch: 330 \tTraining Loss: 0.1877\t Acc: 77.00%\n",
      "Epoch: 331 \tTraining Loss: 0.1879\t Acc: 77.00%\n",
      "Epoch: 332 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 333 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 334 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 335 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 336 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 337 \tTraining Loss: 0.1875\t Acc: 77.00%\n",
      "Epoch: 338 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 339 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 340 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 341 \tTraining Loss: 0.1878\t Acc: 78.00%\n",
      "Epoch: 342 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 343 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 344 \tTraining Loss: 0.1875\t Acc: 77.00%\n",
      "Epoch: 345 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 346 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 347 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 348 \tTraining Loss: 0.1875\t Acc: 77.00%\n",
      "Epoch: 349 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 350 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 351 \tTraining Loss: 0.1877\t Acc: 78.00%\n",
      "Epoch: 352 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 353 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 354 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 355 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 356 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 357 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 358 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 359 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 360 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 361 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 362 \tTraining Loss: 0.1877\t Acc: 79.00%\n",
      "Epoch: 363 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 364 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 365 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 366 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 367 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 368 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 369 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 370 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 371 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 372 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 373 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 374 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 375 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 376 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 377 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 378 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 379 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 380 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 381 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 382 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 383 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 384 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 385 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 386 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 387 \tTraining Loss: 0.1874\t Acc: 77.00%\n",
      "Epoch: 388 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 389 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 390 \tTraining Loss: 0.1876\t Acc: 77.00%\n",
      "Epoch: 391 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 392 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 393 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 394 \tTraining Loss: 0.1875\t Acc: 77.00%\n",
      "Epoch: 395 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 396 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 397 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 398 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 399 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 400 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 401 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 402 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 403 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 404 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 405 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 406 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 407 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 408 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 409 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 410 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 411 \tTraining Loss: 0.1875\t Acc: 79.00%\n",
      "Epoch: 412 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 413 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 414 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 415 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 416 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 417 \tTraining Loss: 0.1875\t Acc: 78.00%\n",
      "Epoch: 418 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 419 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 420 \tTraining Loss: 0.1874\t Acc: 77.00%\n",
      "Epoch: 421 \tTraining Loss: 0.1876\t Acc: 79.00%\n",
      "Epoch: 422 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 423 \tTraining Loss: 0.1876\t Acc: 76.00%\n",
      "Epoch: 424 \tTraining Loss: 0.1874\t Acc: 77.00%\n",
      "Epoch: 425 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 426 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 427 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 428 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 429 \tTraining Loss: 0.1874\t Acc: 77.00%\n",
      "Epoch: 430 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 431 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 432 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 433 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 434 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 435 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 436 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 437 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 438 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 439 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 440 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 441 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 442 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 443 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 444 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 445 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 446 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 447 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 448 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 449 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 450 \tTraining Loss: 0.1876\t Acc: 78.00%\n",
      "Epoch: 451 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 452 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 453 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 454 \tTraining Loss: 0.1869\t Acc: 77.00%\n",
      "Epoch: 455 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 456 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 457 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 458 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 459 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 460 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 461 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 462 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 463 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 464 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 465 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 466 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 467 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 468 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 469 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 470 \tTraining Loss: 0.1874\t Acc: 78.00%\n",
      "Epoch: 471 \tTraining Loss: 0.1871\t Acc: 77.00%\n",
      "Epoch: 472 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 473 \tTraining Loss: 0.1873\t Acc: 77.00%\n",
      "Epoch: 474 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 475 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 476 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 477 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 478 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 479 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 480 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 481 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 482 \tTraining Loss: 0.1873\t Acc: 76.00%\n",
      "Epoch: 483 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 484 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 485 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 486 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 487 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 488 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 489 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 490 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 491 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 492 \tTraining Loss: 0.1874\t Acc: 79.00%\n",
      "Epoch: 493 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 494 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 495 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 496 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 497 \tTraining Loss: 0.1872\t Acc: 75.00%\n",
      "Epoch: 498 \tTraining Loss: 0.1871\t Acc: 77.00%\n",
      "Epoch: 499 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 500 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 501 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 502 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 503 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 504 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 505 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 506 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 507 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 508 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 509 \tTraining Loss: 0.1871\t Acc: 77.00%\n",
      "Epoch: 510 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 511 \tTraining Loss: 0.1871\t Acc: 76.00%\n",
      "Epoch: 512 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 513 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 514 \tTraining Loss: 0.1871\t Acc: 76.00%\n",
      "Epoch: 515 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 516 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 517 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 518 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 519 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 520 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 521 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 522 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 523 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 524 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 525 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 526 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 527 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 528 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 529 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 530 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 531 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 532 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 533 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 534 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 535 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 536 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 537 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 538 \tTraining Loss: 0.1868\t Acc: 77.00%\n",
      "Epoch: 539 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 540 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 541 \tTraining Loss: 0.1873\t Acc: 79.00%\n",
      "Epoch: 542 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 543 \tTraining Loss: 0.1867\t Acc: 77.00%\n",
      "Epoch: 544 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 545 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 546 \tTraining Loss: 0.1869\t Acc: 77.00%\n",
      "Epoch: 547 \tTraining Loss: 0.1867\t Acc: 76.00%\n",
      "Epoch: 548 \tTraining Loss: 0.1871\t Acc: 79.00%\n",
      "Epoch: 549 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 550 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 551 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 552 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 553 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 554 \tTraining Loss: 0.1872\t Acc: 79.00%\n",
      "Epoch: 555 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 556 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 557 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 558 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 559 \tTraining Loss: 0.1867\t Acc: 77.00%\n",
      "Epoch: 560 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 561 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 562 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 563 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 564 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 565 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 566 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 567 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 568 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 569 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 570 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 571 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 572 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 573 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 574 \tTraining Loss: 0.1870\t Acc: 79.00%\n",
      "Epoch: 575 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 576 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 577 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 578 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 579 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 580 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 581 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 582 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 583 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 584 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 585 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 586 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 587 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 588 \tTraining Loss: 0.1871\t Acc: 78.00%\n",
      "Epoch: 589 \tTraining Loss: 0.1866\t Acc: 79.00%\n",
      "Epoch: 590 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 591 \tTraining Loss: 0.1871\t Acc: 77.00%\n",
      "Epoch: 592 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 593 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 594 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 595 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 596 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 597 \tTraining Loss: 0.1867\t Acc: 79.00%\n",
      "Epoch: 598 \tTraining Loss: 0.1872\t Acc: 77.00%\n",
      "Epoch: 599 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 600 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 601 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 602 \tTraining Loss: 0.1873\t Acc: 78.00%\n",
      "Epoch: 603 \tTraining Loss: 0.1868\t Acc: 76.00%\n",
      "Epoch: 604 \tTraining Loss: 0.1872\t Acc: 78.00%\n",
      "Epoch: 605 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 606 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 607 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 608 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 609 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 610 \tTraining Loss: 0.1869\t Acc: 79.00%\n",
      "Epoch: 611 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 612 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 613 \tTraining Loss: 0.1869\t Acc: 77.00%\n",
      "Epoch: 614 \tTraining Loss: 0.1868\t Acc: 77.00%\n",
      "Epoch: 615 \tTraining Loss: 0.1869\t Acc: 77.00%\n",
      "Epoch: 616 \tTraining Loss: 0.1870\t Acc: 78.00%\n",
      "Epoch: 617 \tTraining Loss: 0.1867\t Acc: 79.00%\n",
      "Epoch: 618 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 619 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 620 \tTraining Loss: 0.1865\t Acc: 79.00%\n",
      "Epoch: 621 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 622 \tTraining Loss: 0.1870\t Acc: 77.00%\n",
      "Epoch: 623 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 624 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 625 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 626 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 627 \tTraining Loss: 0.1865\t Acc: 78.00%\n",
      "Epoch: 628 \tTraining Loss: 0.1867\t Acc: 77.00%\n",
      "Epoch: 629 \tTraining Loss: 0.1868\t Acc: 78.00%\n",
      "Epoch: 630 \tTraining Loss: 0.1866\t Acc: 77.00%\n",
      "Epoch: 631 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 632 \tTraining Loss: 0.1866\t Acc: 79.00%\n",
      "Epoch: 633 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 634 \tTraining Loss: 0.1865\t Acc: 78.00%\n",
      "Epoch: 635 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 636 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 637 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 638 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 639 \tTraining Loss: 0.1866\t Acc: 76.00%\n",
      "Epoch: 640 \tTraining Loss: 0.1866\t Acc: 79.00%\n",
      "Epoch: 641 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 642 \tTraining Loss: 0.1863\t Acc: 78.00%\n",
      "Epoch: 643 \tTraining Loss: 0.1867\t Acc: 78.00%\n",
      "Epoch: 644 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 645 \tTraining Loss: 0.1868\t Acc: 75.00%\n",
      "Epoch: 646 \tTraining Loss: 0.1866\t Acc: 78.00%\n",
      "Epoch: 647 \tTraining Loss: 0.1869\t Acc: 78.00%\n",
      "Epoch: 648 \tTraining Loss: 0.1868\t Acc: 79.00%\n",
      "Epoch: 649 \tTraining Loss: 0.1864\t Acc: 79.00%\n",
      "Epoch: 650 \tTraining Loss: 0.1863\t Acc: 79.00%\n",
      "Epoch: 651 \tTraining Loss: 0.1865\t Acc: 78.00%\n",
      "Epoch: 652 \tTraining Loss: 0.1863\t Acc: 78.00%\n",
      "Epoch: 653 \tTraining Loss: 0.1866\t Acc: 79.00%\n",
      "Epoch: 654 \tTraining Loss: 0.1863\t Acc: 78.00%\n",
      "Epoch: 655 \tTraining Loss: 0.1866\t Acc: 79.00%\n",
      "Epoch: 656 \tTraining Loss: 0.1860\t Acc: 77.00%\n",
      "Epoch: 657 \tTraining Loss: 0.1861\t Acc: 79.00%\n",
      "Epoch: 658 \tTraining Loss: 0.1862\t Acc: 78.00%\n",
      "Epoch: 659 \tTraining Loss: 0.1864\t Acc: 79.00%\n",
      "Epoch: 660 \tTraining Loss: 0.1861\t Acc: 77.00%\n",
      "Epoch: 661 \tTraining Loss: 0.1862\t Acc: 79.00%\n",
      "Epoch: 662 \tTraining Loss: 0.1859\t Acc: 79.00%\n",
      "Epoch: 663 \tTraining Loss: 0.1858\t Acc: 79.00%\n",
      "Epoch: 664 \tTraining Loss: 0.1859\t Acc: 79.00%\n",
      "Epoch: 665 \tTraining Loss: 0.1856\t Acc: 78.00%\n",
      "Epoch: 666 \tTraining Loss: 0.1856\t Acc: 77.00%\n",
      "Epoch: 667 \tTraining Loss: 0.1855\t Acc: 79.00%\n",
      "Epoch: 668 \tTraining Loss: 0.1853\t Acc: 78.00%\n",
      "Epoch: 669 \tTraining Loss: 0.1854\t Acc: 77.00%\n",
      "Epoch: 670 \tTraining Loss: 0.1845\t Acc: 78.00%\n",
      "Epoch: 671 \tTraining Loss: 0.1851\t Acc: 78.00%\n",
      "Epoch: 672 \tTraining Loss: 0.1849\t Acc: 78.00%\n",
      "Epoch: 673 \tTraining Loss: 0.1847\t Acc: 79.00%\n",
      "Epoch: 674 \tTraining Loss: 0.1844\t Acc: 77.00%\n",
      "Epoch: 675 \tTraining Loss: 0.1844\t Acc: 77.00%\n",
      "Epoch: 676 \tTraining Loss: 0.1842\t Acc: 78.00%\n",
      "Epoch: 677 \tTraining Loss: 0.1838\t Acc: 79.00%\n",
      "Epoch: 678 \tTraining Loss: 0.1847\t Acc: 77.00%\n",
      "Epoch: 679 \tTraining Loss: 0.1840\t Acc: 79.00%\n",
      "Epoch: 680 \tTraining Loss: 0.1843\t Acc: 78.00%\n",
      "Epoch: 681 \tTraining Loss: 0.1836\t Acc: 79.00%\n",
      "Epoch: 682 \tTraining Loss: 0.1835\t Acc: 78.00%\n",
      "Epoch: 683 \tTraining Loss: 0.1832\t Acc: 78.00%\n",
      "Epoch: 684 \tTraining Loss: 0.1834\t Acc: 77.00%\n",
      "Epoch: 685 \tTraining Loss: 0.1834\t Acc: 77.00%\n",
      "Epoch: 686 \tTraining Loss: 0.1829\t Acc: 78.00%\n",
      "Epoch: 687 \tTraining Loss: 0.1833\t Acc: 79.00%\n",
      "Epoch: 688 \tTraining Loss: 0.1829\t Acc: 78.00%\n",
      "Epoch: 689 \tTraining Loss: 0.1836\t Acc: 79.00%\n",
      "Epoch: 690 \tTraining Loss: 0.1829\t Acc: 79.00%\n",
      "Epoch: 691 \tTraining Loss: 0.1830\t Acc: 78.00%\n",
      "Epoch: 692 \tTraining Loss: 0.1833\t Acc: 77.00%\n",
      "Epoch: 693 \tTraining Loss: 0.1830\t Acc: 78.00%\n",
      "Epoch: 694 \tTraining Loss: 0.1832\t Acc: 78.00%\n",
      "Epoch: 695 \tTraining Loss: 0.1827\t Acc: 78.00%\n",
      "Epoch: 696 \tTraining Loss: 0.1826\t Acc: 78.00%\n",
      "Epoch: 697 \tTraining Loss: 0.1830\t Acc: 77.00%\n",
      "Epoch: 698 \tTraining Loss: 0.1837\t Acc: 76.00%\n",
      "Epoch: 699 \tTraining Loss: 0.1837\t Acc: 77.00%\n",
      "Epoch: 700 \tTraining Loss: 0.1841\t Acc: 79.00%\n",
      "Epoch: 701 \tTraining Loss: 0.1842\t Acc: 77.00%\n",
      "Epoch: 702 \tTraining Loss: 0.1823\t Acc: 79.00%\n",
      "Epoch: 703 \tTraining Loss: 0.1822\t Acc: 79.00%\n",
      "Epoch: 704 \tTraining Loss: 0.1826\t Acc: 76.00%\n",
      "Epoch: 705 \tTraining Loss: 0.1823\t Acc: 78.00%\n",
      "Epoch: 706 \tTraining Loss: 0.1825\t Acc: 77.00%\n",
      "Epoch: 707 \tTraining Loss: 0.1826\t Acc: 77.00%\n",
      "Epoch: 708 \tTraining Loss: 0.1826\t Acc: 76.00%\n",
      "Epoch: 709 \tTraining Loss: 0.1828\t Acc: 78.00%\n",
      "Epoch: 710 \tTraining Loss: 0.1823\t Acc: 78.00%\n",
      "Epoch: 711 \tTraining Loss: 0.1820\t Acc: 79.00%\n",
      "Epoch: 712 \tTraining Loss: 0.1820\t Acc: 77.00%\n",
      "Epoch: 713 \tTraining Loss: 0.1824\t Acc: 77.00%\n",
      "Epoch: 714 \tTraining Loss: 0.1828\t Acc: 78.00%\n",
      "Epoch: 715 \tTraining Loss: 0.1823\t Acc: 78.00%\n",
      "Epoch: 716 \tTraining Loss: 0.1823\t Acc: 77.00%\n",
      "Epoch: 717 \tTraining Loss: 0.1821\t Acc: 77.00%\n",
      "Epoch: 718 \tTraining Loss: 0.1818\t Acc: 79.00%\n",
      "Epoch: 719 \tTraining Loss: 0.1824\t Acc: 79.00%\n",
      "Epoch: 720 \tTraining Loss: 0.1825\t Acc: 76.00%\n",
      "Epoch: 721 \tTraining Loss: 0.1822\t Acc: 78.00%\n",
      "Epoch: 722 \tTraining Loss: 0.1821\t Acc: 77.00%\n",
      "Epoch: 723 \tTraining Loss: 0.1820\t Acc: 74.00%\n",
      "Epoch: 724 \tTraining Loss: 0.1818\t Acc: 77.00%\n",
      "Epoch: 725 \tTraining Loss: 0.1823\t Acc: 79.00%\n",
      "Epoch: 726 \tTraining Loss: 0.1816\t Acc: 78.00%\n",
      "Epoch: 727 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 728 \tTraining Loss: 0.1821\t Acc: 78.00%\n",
      "Epoch: 729 \tTraining Loss: 0.1821\t Acc: 77.00%\n",
      "Epoch: 730 \tTraining Loss: 0.1825\t Acc: 77.00%\n",
      "Epoch: 731 \tTraining Loss: 0.1826\t Acc: 79.00%\n",
      "Epoch: 732 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 733 \tTraining Loss: 0.1816\t Acc: 77.00%\n",
      "Epoch: 734 \tTraining Loss: 0.1818\t Acc: 77.00%\n",
      "Epoch: 735 \tTraining Loss: 0.1821\t Acc: 78.00%\n",
      "Epoch: 736 \tTraining Loss: 0.1821\t Acc: 78.00%\n",
      "Epoch: 737 \tTraining Loss: 0.1820\t Acc: 77.00%\n",
      "Epoch: 738 \tTraining Loss: 0.1813\t Acc: 77.00%\n",
      "Epoch: 739 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 740 \tTraining Loss: 0.1811\t Acc: 77.00%\n",
      "Epoch: 741 \tTraining Loss: 0.1819\t Acc: 76.00%\n",
      "Epoch: 742 \tTraining Loss: 0.1820\t Acc: 76.00%\n",
      "Epoch: 743 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 744 \tTraining Loss: 0.1818\t Acc: 77.00%\n",
      "Epoch: 745 \tTraining Loss: 0.1822\t Acc: 78.00%\n",
      "Epoch: 746 \tTraining Loss: 0.1816\t Acc: 79.00%\n",
      "Epoch: 747 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 748 \tTraining Loss: 0.1817\t Acc: 77.00%\n",
      "Epoch: 749 \tTraining Loss: 0.1814\t Acc: 76.00%\n",
      "Epoch: 750 \tTraining Loss: 0.1816\t Acc: 78.00%\n",
      "Epoch: 751 \tTraining Loss: 0.1815\t Acc: 77.00%\n",
      "Epoch: 752 \tTraining Loss: 0.1823\t Acc: 79.00%\n",
      "Epoch: 753 \tTraining Loss: 0.1819\t Acc: 77.00%\n",
      "Epoch: 754 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 755 \tTraining Loss: 0.1815\t Acc: 78.00%\n",
      "Epoch: 756 \tTraining Loss: 0.1817\t Acc: 79.00%\n",
      "Epoch: 757 \tTraining Loss: 0.1822\t Acc: 79.00%\n",
      "Epoch: 758 \tTraining Loss: 0.1819\t Acc: 77.00%\n",
      "Epoch: 759 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 760 \tTraining Loss: 0.1821\t Acc: 78.00%\n",
      "Epoch: 761 \tTraining Loss: 0.1819\t Acc: 79.00%\n",
      "Epoch: 762 \tTraining Loss: 0.1815\t Acc: 79.00%\n",
      "Epoch: 763 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 764 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 765 \tTraining Loss: 0.1821\t Acc: 77.00%\n",
      "Epoch: 766 \tTraining Loss: 0.1819\t Acc: 78.00%\n",
      "Epoch: 767 \tTraining Loss: 0.1823\t Acc: 78.00%\n",
      "Epoch: 768 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 769 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 770 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 771 \tTraining Loss: 0.1818\t Acc: 77.00%\n",
      "Epoch: 772 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 773 \tTraining Loss: 0.1816\t Acc: 76.00%\n",
      "Epoch: 774 \tTraining Loss: 0.1825\t Acc: 77.00%\n",
      "Epoch: 775 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 776 \tTraining Loss: 0.1817\t Acc: 77.00%\n",
      "Epoch: 777 \tTraining Loss: 0.1815\t Acc: 78.00%\n",
      "Epoch: 778 \tTraining Loss: 0.1814\t Acc: 77.00%\n",
      "Epoch: 779 \tTraining Loss: 0.1821\t Acc: 77.00%\n",
      "Epoch: 780 \tTraining Loss: 0.1812\t Acc: 76.00%\n",
      "Epoch: 781 \tTraining Loss: 0.1815\t Acc: 79.00%\n",
      "Epoch: 782 \tTraining Loss: 0.1815\t Acc: 79.00%\n",
      "Epoch: 783 \tTraining Loss: 0.1818\t Acc: 77.00%\n",
      "Epoch: 784 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 785 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 786 \tTraining Loss: 0.1815\t Acc: 79.00%\n",
      "Epoch: 787 \tTraining Loss: 0.1816\t Acc: 79.00%\n",
      "Epoch: 788 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 789 \tTraining Loss: 0.1820\t Acc: 77.00%\n",
      "Epoch: 790 \tTraining Loss: 0.1815\t Acc: 77.00%\n",
      "Epoch: 791 \tTraining Loss: 0.1818\t Acc: 78.00%\n",
      "Epoch: 792 \tTraining Loss: 0.1816\t Acc: 77.00%\n",
      "Epoch: 793 \tTraining Loss: 0.1814\t Acc: 76.00%\n",
      "Epoch: 794 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 795 \tTraining Loss: 0.1819\t Acc: 77.00%\n",
      "Epoch: 796 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 797 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 798 \tTraining Loss: 0.1816\t Acc: 76.00%\n",
      "Epoch: 799 \tTraining Loss: 0.1814\t Acc: 79.00%\n",
      "Epoch: 800 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 801 \tTraining Loss: 0.1814\t Acc: 77.00%\n",
      "Epoch: 802 \tTraining Loss: 0.1812\t Acc: 79.00%\n",
      "Epoch: 803 \tTraining Loss: 0.1816\t Acc: 79.00%\n",
      "Epoch: 804 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 805 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 806 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 807 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 808 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 809 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 810 \tTraining Loss: 0.1815\t Acc: 77.00%\n",
      "Epoch: 811 \tTraining Loss: 0.1812\t Acc: 79.00%\n",
      "Epoch: 812 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 813 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 814 \tTraining Loss: 0.1814\t Acc: 77.00%\n",
      "Epoch: 815 \tTraining Loss: 0.1813\t Acc: 77.00%\n",
      "Epoch: 816 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 817 \tTraining Loss: 0.1808\t Acc: 79.00%\n",
      "Epoch: 818 \tTraining Loss: 0.1813\t Acc: 79.00%\n",
      "Epoch: 819 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 820 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 821 \tTraining Loss: 0.1813\t Acc: 79.00%\n",
      "Epoch: 822 \tTraining Loss: 0.1808\t Acc: 76.00%\n",
      "Epoch: 823 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 824 \tTraining Loss: 0.1817\t Acc: 77.00%\n",
      "Epoch: 825 \tTraining Loss: 0.1813\t Acc: 75.00%\n",
      "Epoch: 826 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 827 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 828 \tTraining Loss: 0.1816\t Acc: 79.00%\n",
      "Epoch: 829 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 830 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 831 \tTraining Loss: 0.1810\t Acc: 79.00%\n",
      "Epoch: 832 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 833 \tTraining Loss: 0.1813\t Acc: 77.00%\n",
      "Epoch: 834 \tTraining Loss: 0.1811\t Acc: 77.00%\n",
      "Epoch: 835 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 836 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 837 \tTraining Loss: 0.1815\t Acc: 77.00%\n",
      "Epoch: 838 \tTraining Loss: 0.1811\t Acc: 75.00%\n",
      "Epoch: 839 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 840 \tTraining Loss: 0.1811\t Acc: 79.00%\n",
      "Epoch: 841 \tTraining Loss: 0.1813\t Acc: 79.00%\n",
      "Epoch: 842 \tTraining Loss: 0.1817\t Acc: 78.00%\n",
      "Epoch: 843 \tTraining Loss: 0.1814\t Acc: 77.00%\n",
      "Epoch: 844 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 845 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 846 \tTraining Loss: 0.1815\t Acc: 78.00%\n",
      "Epoch: 847 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 848 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 849 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 850 \tTraining Loss: 0.1808\t Acc: 76.00%\n",
      "Epoch: 851 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 852 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 853 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 854 \tTraining Loss: 0.1814\t Acc: 78.00%\n",
      "Epoch: 855 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 856 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 857 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 858 \tTraining Loss: 0.1811\t Acc: 79.00%\n",
      "Epoch: 859 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 860 \tTraining Loss: 0.1811\t Acc: 75.00%\n",
      "Epoch: 861 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 862 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 863 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 864 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 865 \tTraining Loss: 0.1812\t Acc: 74.00%\n",
      "Epoch: 866 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 867 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 868 \tTraining Loss: 0.1810\t Acc: 78.00%\n",
      "Epoch: 869 \tTraining Loss: 0.1813\t Acc: 79.00%\n",
      "Epoch: 870 \tTraining Loss: 0.1810\t Acc: 76.00%\n",
      "Epoch: 871 \tTraining Loss: 0.1814\t Acc: 77.00%\n",
      "Epoch: 872 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 873 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 874 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 875 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 876 \tTraining Loss: 0.1812\t Acc: 78.00%\n",
      "Epoch: 877 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 878 \tTraining Loss: 0.1808\t Acc: 76.00%\n",
      "Epoch: 879 \tTraining Loss: 0.1810\t Acc: 76.00%\n",
      "Epoch: 880 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 881 \tTraining Loss: 0.1810\t Acc: 77.00%\n",
      "Epoch: 882 \tTraining Loss: 0.1810\t Acc: 79.00%\n",
      "Epoch: 883 \tTraining Loss: 0.1810\t Acc: 79.00%\n",
      "Epoch: 884 \tTraining Loss: 0.1808\t Acc: 76.00%\n",
      "Epoch: 885 \tTraining Loss: 0.1805\t Acc: 77.00%\n",
      "Epoch: 886 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 887 \tTraining Loss: 0.1808\t Acc: 79.00%\n",
      "Epoch: 888 \tTraining Loss: 0.1810\t Acc: 78.00%\n",
      "Epoch: 889 \tTraining Loss: 0.1811\t Acc: 76.00%\n",
      "Epoch: 890 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 891 \tTraining Loss: 0.1809\t Acc: 79.00%\n",
      "Epoch: 892 \tTraining Loss: 0.1811\t Acc: 78.00%\n",
      "Epoch: 893 \tTraining Loss: 0.1813\t Acc: 78.00%\n",
      "Epoch: 894 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 895 \tTraining Loss: 0.1812\t Acc: 76.00%\n",
      "Epoch: 896 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 897 \tTraining Loss: 0.1811\t Acc: 76.00%\n",
      "Epoch: 898 \tTraining Loss: 0.1809\t Acc: 79.00%\n",
      "Epoch: 899 \tTraining Loss: 0.1810\t Acc: 78.00%\n",
      "Epoch: 900 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 901 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 902 \tTraining Loss: 0.1810\t Acc: 79.00%\n",
      "Epoch: 903 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 904 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 905 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 906 \tTraining Loss: 0.1807\t Acc: 79.00%\n",
      "Epoch: 907 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 908 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 909 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 910 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 911 \tTraining Loss: 0.1808\t Acc: 76.00%\n",
      "Epoch: 912 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 913 \tTraining Loss: 0.1807\t Acc: 76.00%\n",
      "Epoch: 914 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 915 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 916 \tTraining Loss: 0.1809\t Acc: 76.00%\n",
      "Epoch: 917 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 918 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 919 \tTraining Loss: 0.1808\t Acc: 75.00%\n",
      "Epoch: 920 \tTraining Loss: 0.1811\t Acc: 77.00%\n",
      "Epoch: 921 \tTraining Loss: 0.1805\t Acc: 75.00%\n",
      "Epoch: 922 \tTraining Loss: 0.1810\t Acc: 77.00%\n",
      "Epoch: 923 \tTraining Loss: 0.1804\t Acc: 76.00%\n",
      "Epoch: 924 \tTraining Loss: 0.1810\t Acc: 76.00%\n",
      "Epoch: 925 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 926 \tTraining Loss: 0.1810\t Acc: 77.00%\n",
      "Epoch: 927 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 928 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 929 \tTraining Loss: 0.1809\t Acc: 79.00%\n",
      "Epoch: 930 \tTraining Loss: 0.1804\t Acc: 76.00%\n",
      "Epoch: 931 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 932 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 933 \tTraining Loss: 0.1809\t Acc: 75.00%\n",
      "Epoch: 934 \tTraining Loss: 0.1810\t Acc: 79.00%\n",
      "Epoch: 935 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 936 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 937 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 938 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 939 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 940 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 941 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 942 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 943 \tTraining Loss: 0.1811\t Acc: 77.00%\n",
      "Epoch: 944 \tTraining Loss: 0.1805\t Acc: 79.00%\n",
      "Epoch: 945 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 946 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 947 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 948 \tTraining Loss: 0.1807\t Acc: 77.00%\n",
      "Epoch: 949 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 950 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 951 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 952 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 953 \tTraining Loss: 0.1810\t Acc: 76.00%\n",
      "Epoch: 954 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 955 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 956 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 957 \tTraining Loss: 0.1809\t Acc: 77.00%\n",
      "Epoch: 958 \tTraining Loss: 0.1808\t Acc: 77.00%\n",
      "Epoch: 959 \tTraining Loss: 0.1802\t Acc: 76.00%\n",
      "Epoch: 960 \tTraining Loss: 0.1810\t Acc: 77.00%\n",
      "Epoch: 961 \tTraining Loss: 0.1800\t Acc: 77.00%\n",
      "Epoch: 962 \tTraining Loss: 0.1801\t Acc: 79.00%\n",
      "Epoch: 963 \tTraining Loss: 0.1804\t Acc: 78.00%\n",
      "Epoch: 964 \tTraining Loss: 0.1806\t Acc: 79.00%\n",
      "Epoch: 965 \tTraining Loss: 0.1807\t Acc: 79.00%\n",
      "Epoch: 966 \tTraining Loss: 0.1805\t Acc: 75.00%\n",
      "Epoch: 967 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 968 \tTraining Loss: 0.1806\t Acc: 79.00%\n",
      "Epoch: 969 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 970 \tTraining Loss: 0.1804\t Acc: 77.00%\n",
      "Epoch: 971 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 972 \tTraining Loss: 0.1808\t Acc: 78.00%\n",
      "Epoch: 973 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 974 \tTraining Loss: 0.1810\t Acc: 76.00%\n",
      "Epoch: 975 \tTraining Loss: 0.1809\t Acc: 78.00%\n",
      "Epoch: 976 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 977 \tTraining Loss: 0.1803\t Acc: 77.00%\n",
      "Epoch: 978 \tTraining Loss: 0.1805\t Acc: 76.00%\n",
      "Epoch: 979 \tTraining Loss: 0.1800\t Acc: 78.00%\n",
      "Epoch: 980 \tTraining Loss: 0.1804\t Acc: 77.00%\n",
      "Epoch: 981 \tTraining Loss: 0.1801\t Acc: 79.00%\n",
      "Epoch: 982 \tTraining Loss: 0.1805\t Acc: 78.00%\n",
      "Epoch: 983 \tTraining Loss: 0.1807\t Acc: 79.00%\n",
      "Epoch: 984 \tTraining Loss: 0.1807\t Acc: 78.00%\n",
      "Epoch: 985 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 986 \tTraining Loss: 0.1802\t Acc: 78.00%\n",
      "Epoch: 987 \tTraining Loss: 0.1805\t Acc: 78.00%\n",
      "Epoch: 988 \tTraining Loss: 0.1804\t Acc: 78.00%\n",
      "Epoch: 989 \tTraining Loss: 0.1806\t Acc: 76.00%\n",
      "Epoch: 990 \tTraining Loss: 0.1812\t Acc: 77.00%\n",
      "Epoch: 991 \tTraining Loss: 0.1803\t Acc: 74.00%\n",
      "Epoch: 992 \tTraining Loss: 0.1806\t Acc: 78.00%\n",
      "Epoch: 993 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 994 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 995 \tTraining Loss: 0.1804\t Acc: 79.00%\n",
      "Epoch: 996 \tTraining Loss: 0.1806\t Acc: 77.00%\n",
      "Epoch: 997 \tTraining Loss: 0.1802\t Acc: 78.00%\n",
      "Epoch: 998 \tTraining Loss: 0.1802\t Acc: 77.00%\n",
      "Epoch: 999 \tTraining Loss: 0.1803\t Acc: 78.00%\n",
      "Epoch: 1000 \tTraining Loss: 0.1801\t Acc: 77.00%\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# Set the training mode ON -> Activate Dropout Layers\n",
    "model.train() # prepare model for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data,target in train_loader:\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "        #print(\"Target = \",target[0].item())\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        predicted = (torch.round(output.data[0]))\n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = loss_fn(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average training loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    # Avg Accuracy\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    # Put them in their list\n",
    "    train_acc_list.append(accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        accuracy\n",
    "        ))\n",
    "    # Move to next epoch\n",
    "    epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,train_loss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Number of Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,train_acc_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.78\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "model.eval() # Required for Evaluation/Test\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Convert our images and labels to Variables to accumulate Gradients\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "\n",
    "        # Predict Output\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(output, target)\n",
    "        val_loss += loss.item()*data.size(0)\n",
    "        # Get predictions from the maximum value\n",
    "        predicted = (torch.round(output.data[0]))\n",
    "\n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "    \n",
    "    # calculate average training loss and accuracy over an epoch\n",
    "    val_loss = val_loss/len(test_loader.dataset)\n",
    "    accuracy = 100 * correct/ float(total)\n",
    "print(\"Accuracy = \",accuracy.item() * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kaggle_test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-217-03bd4c6b168a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkaggle_test_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkaggle_test_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'kaggle_test_set' is not defined"
     ]
    }
   ],
   "source": [
    "kaggle_test_set = Variable(kaggle_test_set).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kaggle_test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-c3fcde7dd017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkaggle_test_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kaggle_test_set' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Set Evaluation Mode ON -> Turn Off Dropout\n",
    "model.eval() # Required for Evaluation/Test\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in kaggle_test_set:\n",
    "        output = model(data)\n",
    "        pred = int((torch.round(output.data[0])).item())\n",
    "        results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
